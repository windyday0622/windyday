{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YyBVgm0UqeIUd22q2llV_LposUz_-M-6",
      "authorship_tag": "ABX9TyNG3WOVZDm+ZM1vyRqzHsgh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/windyday0622/windyday/blob/main/m6_%EB%94%A5%EB%9F%AC%EB%8B%9D/%20m6_08_pytorch_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. mnist 데이터 셋에 대하여 Pytorch를 적용하여 모델 구성 변경, 조기 학습 중단을 수행하고 Best model을 저장한 후 다시불러와서 테스트 데이터로 평가한 결과를 출력하세요."
      ],
      "metadata": {
        "id": "7dQ0M0hVdOy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.max(outputs, 1)\n",
        "\n",
        "import torch\n",
        "\n",
        "# 예시 출력 텐서\n",
        "outputs = torch.tensor([[0.1, 0.3, 0.6], [0.2, 0.7, 0.1]])\n",
        "\n",
        "# 가장 큰 값을 가지는 클래스 인덱스를 추출\n",
        "_, predicted = torch.max(outputs, 1) # 각 행에 대해 최대값\n",
        "\n",
        "print(predicted)  # tensor([2, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5fs1kY2rhvM",
        "outputId": "08da2ade-0892-40e3-cfa8-c16e1286dc59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4jX1d6gdBXS",
        "outputId": "d7c5d825-8298-49c8-b7c1-75aba0bd689b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.003122352917989095, Val Loss: 2.30473053201716\n",
            "Validation loss decreased (inf --> 2.304731) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.006184267044067383, Val Loss: 2.2941723592737886\n",
            "Validation loss decreased (2.304731 --> 2.294172) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.009255009651184082, Val Loss: 2.281798168699792\n",
            "Validation loss decreased (2.294172 --> 2.281798) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.01231139850616455, Val Loss: 2.268167220531626\n",
            "Validation loss decreased (2.281798 --> 2.268167) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.015325480461120606, Val Loss: 2.25461801062239\n",
            "Validation loss decreased (2.268167 --> 2.254618) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.018338297526041665, Val Loss: 2.2382638061300235\n",
            "Validation loss decreased (2.254618 --> 2.238264) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.021316164970397948, Val Loss: 2.2201491759178484\n",
            "Validation loss decreased (2.238264 --> 2.220149) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.02430680465698242, Val Loss: 2.1969154299573694\n",
            "Validation loss decreased (2.220149 --> 2.196915) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.027184892654418945, Val Loss: 2.171613116213616\n",
            "Validation loss decreased (2.196915 --> 2.171613) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.030133110364278157, Val Loss: 2.145134834533042\n",
            "Validation loss decreased (2.171613 --> 2.145135) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.03299935340881348, Val Loss: 2.1156689689514483\n",
            "Validation loss decreased (2.145135 --> 2.115669) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.0358654146194458, Val Loss: 2.0847413413068083\n",
            "Validation loss decreased (2.115669 --> 2.084741) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.03861341857910156, Val Loss: 2.052427013503744\n",
            "Validation loss decreased (2.084741 --> 2.052427) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.04131093947092692, Val Loss: 2.018418235347626\n",
            "Validation loss decreased (2.052427 --> 2.018418) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.04394299379984538, Val Loss: 1.9818577379622357\n",
            "Validation loss decreased (2.018418 --> 1.981858) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.046509655793507895, Val Loss: 1.943202090390185\n",
            "Validation loss decreased (1.981858 --> 1.943202) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.049077386856079104, Val Loss: 1.9000889462359407\n",
            "Validation loss decreased (1.943202 --> 1.900089) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.05160453240076701, Val Loss: 1.8545777569425868\n",
            "Validation loss decreased (1.900089 --> 1.854578) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.05405464442571004, Val Loss: 1.806048002014769\n",
            "Validation loss decreased (1.854578 --> 1.806048) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.056355267842610676, Val Loss: 1.753638043682626\n",
            "Validation loss decreased (1.806048 --> 1.753638) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.0586663810412089, Val Loss: 1.6988353393179305\n",
            "Validation loss decreased (1.753638 --> 1.698835) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.06078482596079508, Val Loss: 1.6436313650709518\n",
            "Validation loss decreased (1.698835 --> 1.643631) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.06296570460001627, Val Loss: 1.5907839219620887\n",
            "Validation loss decreased (1.643631 --> 1.590784) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.06500092458724975, Val Loss: 1.5382952639397154\n",
            "Validation loss decreased (1.590784 --> 1.538295) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.06731547609965007, Val Loss: 1.4787098334190694\n",
            "Validation loss decreased (1.538295 --> 1.478710) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.06917721875508627, Val Loss: 1.408433899600455\n",
            "Validation loss decreased (1.478710 --> 1.408434) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.07091559298833211, Val Loss: 1.338884879934027\n",
            "Validation loss decreased (1.408434 --> 1.338885) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.07277904176712036, Val Loss: 1.265355589541983\n",
            "Validation loss decreased (1.338885 --> 1.265356) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.0745359894434611, Val Loss: 1.2100073774444295\n",
            "Validation loss decreased (1.265356 --> 1.210007) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.07609410762786865, Val Loss: 1.174281526753243\n",
            "Validation loss decreased (1.210007 --> 1.174282) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.07743740272521972, Val Loss: 1.1387740773089388\n",
            "Validation loss decreased (1.174282 --> 1.138774) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.07895117505391439, Val Loss: 1.0694016059662432\n",
            "Validation loss decreased (1.138774 --> 1.069402) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.08021172523498535, Val Loss: 0.9809556340283536\n",
            "Validation loss decreased (1.069402 --> 0.980956) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.0814981764157613, Val Loss: 0.9033894741788824\n",
            "Validation loss decreased (0.980956 --> 0.903389) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.08285040616989135, Val Loss: 0.8580823708721932\n",
            "Validation loss decreased (0.903389 --> 0.858082) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.0838376518090566, Val Loss: 0.8346484702952365\n",
            "Validation loss decreased (0.858082 --> 0.834648) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.08492472275098165, Val Loss: 0.8045539545251968\n",
            "Validation loss decreased (0.834648 --> 0.804554) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.08595943442980448, Val Loss: 0.7437318560290844\n",
            "Validation loss decreased (0.804554 --> 0.743732) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.08705273048082987, Val Loss: 0.6925854667070064\n",
            "Validation loss decreased (0.743732 --> 0.692585) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.08796129179000854, Val Loss: 0.6697387571664567\n",
            "Validation loss decreased (0.692585 --> 0.669739) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.08871225214004516, Val Loss: 0.6787755904679603\n",
            "Epoch 1, Train Loss: 0.08952752939860026, Val Loss: 0.6618738325035318\n",
            "Validation loss decreased (0.669739 --> 0.661874) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.09047413516044617, Val Loss: 0.6345114405167863\n",
            "Validation loss decreased (0.661874 --> 0.634511) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.09126765314737956, Val Loss: 0.6058218751816039\n",
            "Validation loss decreased (0.634511 --> 0.605822) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.0921922873655955, Val Loss: 0.5797386478553427\n",
            "Validation loss decreased (0.605822 --> 0.579739) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.092964062611262, Val Loss: 0.5793289323436454\n",
            "Validation loss decreased (0.579739 --> 0.579329) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.0938447322845459, Val Loss: 0.5738557293377025\n",
            "Validation loss decreased (0.579329 --> 0.573856) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.09486247237523397, Val Loss: 0.5260720530405958\n",
            "Validation loss decreased (0.573856 --> 0.526072) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.09555455795923869, Val Loss: 0.504657213199646\n",
            "Validation loss decreased (0.526072 --> 0.504657) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.09626925468444825, Val Loss: 0.5034281684363142\n",
            "Validation loss decreased (0.504657 --> 0.503428) \t  Saving The Model\n",
            "Epoch 1, Train Loss: 0.09663229548931122, Val Loss: 0.5225081653036969\n",
            "Epoch 1, Train Loss: 0.09725471210479736, Val Loss: 0.5576000758942138\n",
            "Epoch 1, Train Loss: 0.0979576141834259, Val Loss: 0.5646311962541114\n",
            "Epoch 1, Train Loss: 0.09843078541755676, Val Loss: 0.5723110069619849\n",
            "Epoch 1, Train Loss: 0.09888919393221537, Val Loss: 0.6007940143029741\n",
            "Early stopping triggered\n",
            "Epoch 2, Train Loss: 0.0005880664984385173, Val Loss: 0.594845195241431\n",
            "Early stopping triggered\n",
            "Epoch 3, Train Loss: 0.0007418420314788818, Val Loss: 0.522899515768315\n",
            "Early stopping triggered\n",
            "Epoch 4, Train Loss: 0.0005736527840296428, Val Loss: 0.45720121819288173\n",
            "Validation loss decreased (0.503428 --> 0.457201) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.0012997051477432252, Val Loss: 0.44674587899700124\n",
            "Validation loss decreased (0.457201 --> 0.446746) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.0017461589574813844, Val Loss: 0.4777989179847088\n",
            "Epoch 4, Train Loss: 0.0024691315094629925, Val Loss: 0.4694498864577172\n",
            "Epoch 4, Train Loss: 0.0029865467151006064, Val Loss: 0.46229037967451075\n",
            "Epoch 4, Train Loss: 0.003429715673128764, Val Loss: 0.4497280554409991\n",
            "Epoch 4, Train Loss: 0.0039116532405217485, Val Loss: 0.4410871889521467\n",
            "Validation loss decreased (0.446746 --> 0.441087) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.004643111427625021, Val Loss: 0.43604495154416306\n",
            "Validation loss decreased (0.441087 --> 0.436045) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.005179483970006307, Val Loss: 0.43960654925792775\n",
            "Epoch 4, Train Loss: 0.005715809305508931, Val Loss: 0.4347331185448677\n",
            "Validation loss decreased (0.436045 --> 0.434733) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.006202061374982198, Val Loss: 0.42134235902352535\n",
            "Validation loss decreased (0.434733 --> 0.421342) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.006989445408185323, Val Loss: 0.4031616452526539\n",
            "Validation loss decreased (0.421342 --> 0.403162) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.007727119326591492, Val Loss: 0.40418365201417433\n",
            "Epoch 4, Train Loss: 0.008317506869633992, Val Loss: 0.40877063065133196\n",
            "Epoch 4, Train Loss: 0.00876114046573639, Val Loss: 0.4095596124200111\n",
            "Epoch 4, Train Loss: 0.009118311206499736, Val Loss: 0.3933090641301997\n",
            "Validation loss decreased (0.403162 --> 0.393309) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.009486929337183635, Val Loss: 0.3739070145690695\n",
            "Validation loss decreased (0.393309 --> 0.373907) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.010075848261515299, Val Loss: 0.36965750799851216\n",
            "Validation loss decreased (0.373907 --> 0.369658) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.010521559635798136, Val Loss: 0.37977367837695364\n",
            "Epoch 4, Train Loss: 0.010922699689865112, Val Loss: 0.389134397452816\n",
            "Epoch 4, Train Loss: 0.01134118906656901, Val Loss: 0.3889556778396698\n",
            "Epoch 4, Train Loss: 0.011783147970835368, Val Loss: 0.376293364674487\n",
            "Epoch 4, Train Loss: 0.012258134007453918, Val Loss: 0.35714080232255\n",
            "Validation loss decreased (0.369658 --> 0.357141) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.012732729037602742, Val Loss: 0.34109165551180537\n",
            "Validation loss decreased (0.357141 --> 0.341092) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.01309754200776418, Val Loss: 0.32983385050233377\n",
            "Validation loss decreased (0.341092 --> 0.329834) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.013490614215532938, Val Loss: 0.3221312940437743\n",
            "Validation loss decreased (0.329834 --> 0.322131) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.013706707974274953, Val Loss: 0.3216283546483263\n",
            "Validation loss decreased (0.322131 --> 0.321628) \t  Saving The Model\n",
            "Epoch 4, Train Loss: 0.014311678349971772, Val Loss: 0.3262000328841362\n",
            "Epoch 4, Train Loss: 0.014616007308165231, Val Loss: 0.32661470548903687\n",
            "Epoch 4, Train Loss: 0.014920077979564668, Val Loss: 0.327825868858936\n",
            "Epoch 4, Train Loss: 0.015245214839776358, Val Loss: 0.3278035909095977\n",
            "Epoch 4, Train Loss: 0.015808143238226574, Val Loss: 0.3260830299968415\n",
            "Early stopping triggered\n",
            "Epoch 5, Train Loss: 0.0005522101720174153, Val Loss: 0.3157976396540378\n",
            "Validation loss decreased (0.321628 --> 0.315798) \t  Saving The Model\n",
            "Epoch 5, Train Loss: 0.0011244843403498332, Val Loss: 0.30280074663460255\n",
            "Validation loss decreased (0.315798 --> 0.302801) \t  Saving The Model\n",
            "Epoch 5, Train Loss: 0.0017136589686075846, Val Loss: 0.29297075991300825\n",
            "Validation loss decreased (0.302801 --> 0.292971) \t  Saving The Model\n",
            "Epoch 5, Train Loss: 0.00197734397649765, Val Loss: 0.29123570488646944\n",
            "Validation loss decreased (0.292971 --> 0.291236) \t  Saving The Model\n",
            "Epoch 5, Train Loss: 0.002308439572652181, Val Loss: 0.29597205795506215\n",
            "Epoch 5, Train Loss: 0.002744562268257141, Val Loss: 0.30305936472847106\n",
            "Epoch 5, Train Loss: 0.003237174073855082, Val Loss: 0.3105127430976705\n",
            "Epoch 5, Train Loss: 0.0037813604275385537, Val Loss: 0.3087181049458524\n",
            "Epoch 5, Train Loss: 0.004499177972475688, Val Loss: 0.29185201275221845\n",
            "Early stopping triggered\n",
            "Epoch 6, Train Loss: 0.0004048194885253906, Val Loss: 0.2781550838671466\n",
            "Validation loss decreased (0.291236 --> 0.278155) \t  Saving The Model\n",
            "Epoch 6, Train Loss: 0.0009696777661641439, Val Loss: 0.2730164449820493\n",
            "Validation loss decreased (0.278155 --> 0.273016) \t  Saving The Model\n",
            "Epoch 6, Train Loss: 0.0012560447851816814, Val Loss: 0.2792194613671683\n",
            "Epoch 6, Train Loss: 0.0016273802916208903, Val Loss: 0.29548946173584206\n",
            "Epoch 6, Train Loss: 0.0019203245043754578, Val Loss: 0.3060550900532844\n",
            "Epoch 6, Train Loss: 0.0022499534289042154, Val Loss: 0.31083649028330407\n",
            "Epoch 6, Train Loss: 0.0026408105691274005, Val Loss: 0.30921505387634673\n",
            "Early stopping triggered\n",
            "Epoch 7, Train Loss: 0.0005490235884984334, Val Loss: 0.28862497888188415\n",
            "Early stopping triggered\n",
            "Epoch 8, Train Loss: 0.00038910969098409015, Val Loss: 0.26961357713221235\n",
            "Validation loss decreased (0.273016 --> 0.269614) \t  Saving The Model\n",
            "Epoch 8, Train Loss: 0.0008310775756835937, Val Loss: 0.25494588364629034\n",
            "Validation loss decreased (0.269614 --> 0.254946) \t  Saving The Model\n",
            "Epoch 8, Train Loss: 0.001140092412630717, Val Loss: 0.2610636464458831\n",
            "Epoch 8, Train Loss: 0.001409699320793152, Val Loss: 0.27229265179088774\n",
            "Epoch 8, Train Loss: 0.0015841176708539326, Val Loss: 0.28294669206630674\n",
            "Epoch 8, Train Loss: 0.0020247074564297995, Val Loss: 0.2916378177543904\n",
            "Epoch 8, Train Loss: 0.0022511863112449648, Val Loss: 0.2858390184713805\n",
            "Early stopping triggered\n",
            "Epoch 9, Train Loss: 0.00016542136669158935, Val Loss: 0.27856332313348636\n",
            "Early stopping triggered\n",
            "Epoch 10, Train Loss: 0.0003183540503184001, Val Loss: 0.26449570491751456\n",
            "Early stopping triggered\n",
            "Epoch 11, Train Loss: 0.00025541178385416667, Val Loss: 0.25798635168912565\n",
            "Early stopping triggered\n",
            "Epoch 12, Train Loss: 0.00042304742336273193, Val Loss: 0.2605801290891906\n",
            "Early stopping triggered\n",
            "Epoch 13, Train Loss: 0.00023286894957224528, Val Loss: 0.26108105880941485\n",
            "Early stopping triggered\n",
            "Epoch 14, Train Loss: 0.000655630350112915, Val Loss: 0.2535658791423478\n",
            "Validation loss decreased (0.254946 --> 0.253566) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.0010674200852711995, Val Loss: 0.25227686794514353\n",
            "Validation loss decreased (0.253566 --> 0.252277) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.0012826294501622518, Val Loss: 0.2511059791245993\n",
            "Validation loss decreased (0.252277 --> 0.251106) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.0015390430291493734, Val Loss: 0.2490031468741437\n",
            "Validation loss decreased (0.251106 --> 0.249003) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.001865948736667633, Val Loss: 0.24346707178715696\n",
            "Validation loss decreased (0.249003 --> 0.243467) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.00212757142384847, Val Loss: 0.24181985692616473\n",
            "Validation loss decreased (0.243467 --> 0.241820) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.0022984726230303445, Val Loss: 0.24056527878534287\n",
            "Validation loss decreased (0.241820 --> 0.240565) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.002440376490354538, Val Loss: 0.2399017800517539\n",
            "Validation loss decreased (0.240565 --> 0.239902) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.0026951019068559012, Val Loss: 0.23623578979613932\n",
            "Validation loss decreased (0.239902 --> 0.236236) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.002954440067211787, Val Loss: 0.23439450661077144\n",
            "Validation loss decreased (0.236236 --> 0.234395) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.0032655140459537505, Val Loss: 0.23088947691498918\n",
            "Validation loss decreased (0.234395 --> 0.230889) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.0034890626172224683, Val Loss: 0.22986907460429568\n",
            "Validation loss decreased (0.230889 --> 0.229869) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.003907049010197321, Val Loss: 0.22916167556367656\n",
            "Validation loss decreased (0.229869 --> 0.229162) \t  Saving The Model\n",
            "Epoch 14, Train Loss: 0.004362333605686823, Val Loss: 0.2323893446356375\n",
            "Epoch 14, Train Loss: 0.004673622558514277, Val Loss: 0.24110185119145094\n",
            "Epoch 14, Train Loss: 0.005181731293598811, Val Loss: 0.24584015462785325\n",
            "Epoch 14, Train Loss: 0.005758590290943781, Val Loss: 0.2482783261765825\n",
            "Epoch 14, Train Loss: 0.006127017766237259, Val Loss: 0.24300996479677392\n",
            "Early stopping triggered\n",
            "Epoch 15, Train Loss: 0.0003533241351445516, Val Loss: 0.2383498199720015\n",
            "Early stopping triggered\n",
            "Epoch 16, Train Loss: 0.0002643732825915019, Val Loss: 0.23242592106157162\n",
            "Early stopping triggered\n",
            "Epoch 17, Train Loss: 0.00035477936267852783, Val Loss: 0.2322679184614978\n",
            "Early stopping triggered\n",
            "Epoch 18, Train Loss: 0.00039602156480153403, Val Loss: 0.22850430582432038\n",
            "Validation loss decreased (0.229162 --> 0.228504) \t  Saving The Model\n",
            "Epoch 18, Train Loss: 0.000642699122428894, Val Loss: 0.22734184230261661\n",
            "Validation loss decreased (0.228504 --> 0.227342) \t  Saving The Model\n",
            "Epoch 18, Train Loss: 0.0008769514362017314, Val Loss: 0.23170104329573346\n",
            "Epoch 18, Train Loss: 0.0012699119051297505, Val Loss: 0.2313551992336486\n",
            "Epoch 18, Train Loss: 0.0014737037817637126, Val Loss: 0.22954624019404676\n",
            "Epoch 18, Train Loss: 0.00181037970383962, Val Loss: 0.22487957601217515\n",
            "Validation loss decreased (0.227342 --> 0.224880) \t  Saving The Model\n",
            "Epoch 18, Train Loss: 0.0019558762907981872, Val Loss: 0.2212878027732702\n",
            "Validation loss decreased (0.224880 --> 0.221288) \t  Saving The Model\n",
            "Epoch 18, Train Loss: 0.0021732224027315775, Val Loss: 0.22091527255450158\n",
            "Validation loss decreased (0.221288 --> 0.220915) \t  Saving The Model\n",
            "Epoch 18, Train Loss: 0.0024240355292956036, Val Loss: 0.21860765713326474\n",
            "Validation loss decreased (0.220915 --> 0.218608) \t  Saving The Model\n",
            "Epoch 18, Train Loss: 0.0027308216492335003, Val Loss: 0.21622782830703766\n",
            "Validation loss decreased (0.218608 --> 0.216228) \t  Saving The Model\n",
            "Epoch 18, Train Loss: 0.0029256653587023417, Val Loss: 0.21386782048230477\n",
            "Validation loss decreased (0.216228 --> 0.213868) \t  Saving The Model\n",
            "Epoch 18, Train Loss: 0.003252822458744049, Val Loss: 0.2092153080640004\n",
            "Validation loss decreased (0.213868 --> 0.209215) \t  Saving The Model\n",
            "Epoch 18, Train Loss: 0.003825389007727305, Val Loss: 0.20322104042118533\n",
            "Validation loss decreased (0.209215 --> 0.203221) \t  Saving The Model\n",
            "Epoch 18, Train Loss: 0.004123281061649322, Val Loss: 0.20394285895088884\n",
            "Epoch 18, Train Loss: 0.004439400672912598, Val Loss: 0.213337166611343\n",
            "Epoch 18, Train Loss: 0.0047191506624221806, Val Loss: 0.22179766169729384\n",
            "Epoch 18, Train Loss: 0.005111812114715576, Val Loss: 0.22342891113317392\n",
            "Epoch 18, Train Loss: 0.005320520639419556, Val Loss: 0.21892420705804166\n",
            "Early stopping triggered\n",
            "Epoch 19, Train Loss: 0.00036634409427642825, Val Loss: 0.21789332780432194\n",
            "Early stopping triggered\n",
            "Epoch 20, Train Loss: 0.00029180743296941124, Val Loss: 0.21896051242947578\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4e61f6ab8f83>:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the 10000 test images: 94.24 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# MNIST 데이터셋을 위한 전처리 과정 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# 훈련 데이터셋을 훈련 및 검증 세트로 분할\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# 데이터 로더 정의\n",
        "trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "testloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 모델 아키텍처 정의\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(2880, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = MyModel()\n",
        "\n",
        "# 손실 함수 및 최적화 알고리즘 지정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# 모델 훈련\n",
        "best_val_loss = float('inf')\n",
        "patience, trials = 5, 0\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # 검증 단계\n",
        "        val_loss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valloader:\n",
        "                output = model(inputs)\n",
        "                loss = criterion(output, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(valloader)\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {running_loss / len(trainloader)}, Val Loss: {val_loss}\")\n",
        "\n",
        "        # 검증 손실이 개선되었는지 확인하고 모델 저장\n",
        "        if val_loss < best_val_loss:\n",
        "            print(f'Validation loss decreased ({best_val_loss:.6f} --> {val_loss:.6f}) \\t  Saving The Model')\n",
        "            best_val_loss = val_loss\n",
        "            trials = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            trials += 1\n",
        "            if trials >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "# 최고의 모델을 불러와서 평가\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# 모델 평가\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on the 10000 test images: {accuracy} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- torch: 이것은 메인 PyTorch 라이브러리입니다. 여기에는 GPU를 통한 가속 텐서 계산 지원, 신경망 훈련을 용이하게 하는 자동 차별화, 모델 구축 및 훈련을 위한 다양한 유틸리티가 포함됩니다.\n",
        "- torch.nn: 레이어, 활성화 함수, 손실 함수와 같은 신경망의 구성 요소를 제공하는 PyTorch의 하위 모듈입니다. 신경망의 아키텍처를 정의하는 데 필수적입니다.\n",
        "- torch.nn.function: 이 모듈에는 torch.nn 레이어에서 사용되는 기능이 포함되어 있습니다. 입력 데이터 및 가중치에 이러한 함수를 직접 사용할 수 있으므로 일부 작업에 더 많은 유연성을 제공합니다. 여기에는 활성화, 손실 계산 및 상태(즉, 가중치)를 유지하지 않는 다양한 기타 작업을 위한 함수가 포함됩니다.\n",
        "- torch.optim: 이 하위 모듈은 SGD(Stochastic Gradient Descent), Adam 등과 같은 신경망 훈련을 위한 최적화 알고리즘을 제공합니다. 이러한 최적화 프로그램은 계산된 기울기를 기반으로 네트워크의 가중치를 업데이트하는 데 사용됩니다.\n",
        "- torchvision: 이미지 데이터 작업을 위한 유틸리티를 제공하는 PyTorch 프로젝트의 패키지입니다. 여기에는 사전 정의된 데이터세트(예: MNIST, CIFAR10, FashionMNIST), 모델 아키텍처(예: ResNet, AlexNet) 및 전처리를 위한 일반적인 이미지 변환이 포함됩니다.\n",
        "- torchvision.transforms: 일반적인 이미지 변환을 제공하는 torchvision 내의 모듈입니다. 이는 이미지를 신경망에 공급하기 전에 데이터 증대 및 이미지 전처리에 사용될 수 있습니다. 예로는 크기 조정, 정규화, 텐서로 변환 등이 있습니다.\n",
        "- SubsetRandomSampler: 교체 없이 데이터세트에서 요소를 무작위로 샘플링하는 데 사용되는 도구입니다. 데이터 세트를 훈련 및 검증/테스트 세트로 분할하거나 모델 훈련을 위해 사용자 정의 데이터 샘플링 전략을 구현하려는 경우에 특히 유용"
      ],
      "metadata": {
        "id": "DJKAWrU7zvLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. PyTorch를 사용하여 FashionMNIST 데이터세트에 대한 분류 모델링 및 평가를 다음과 같은 단계로 수행하세요.\n",
        "- 1단계: 신경망 모델 정의\n",
        "- 2단계: FashionMNIST 데이터셋 로드\n",
        "- 3단계: 네트워크, 손실 함수, 최적화 알고리즘 초기화\n",
        "- 4단계: 조기 종료를 포함한 모델 학습 및 best model 저장\n",
        "- 5단계: best model을 로드하고 테스트 데이터셋으로 평가"
      ],
      "metadata": {
        "id": "HaKK8VXhzxlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# 1단계: 신경망 모델 정의\n",
        "# Net 클래스는 nn.Module을 상속받아 만들어진 사용자 정의 신경망 모델로, FashionMNIST 데이터셋의 이미지 분류를 위해 설계\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 첫 번째 합성곱 레이어: 입력 채널 1개, 출력 채널 6개, 커널 크기 5x5\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        # 두 번째 합성곱 레이어: 입력 채널 6개, 출력 채널 16개, 커널 크기 5x5\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # 전결합 레이어\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # 16개의 채널과 4x4 이미지 크기\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 첫 번째 합성곱, ReLU 활성화, 맥스 풀링\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        # 두 번째 합성곱, ReLU 활성화, 맥스 풀링\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # 입력 데이터를 1차원으로 펼침\n",
        "        x = self.flatten(x)\n",
        "        # x = x.view(-1, 16 * 4 * 4)\n",
        "        # 전결합 레이어와 ReLU 활성화 함수 적용\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # 마지막 레이어 출력\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 2단계: FashionMNIST 데이터셋 로드\n",
        "# torchvision 라이브러리를 사용하여 FashionMNIST 데이터셋을 다운로드하고, 데이터를 전처리하기 위한 변환(transform)을 설정하는 과정\n",
        "\n",
        "# transforms.Compose는 여러 전처리 단계를 하나로 묶어주는 역할\n",
        "# transforms.ToTensor(): 이미지를 PyTorch 텐서로 변환. 이미지의 픽셀 값 범위가 0에서 255 사이의 정수에서 0.0에서 1.0 사이의 부동소수점으로 변경\n",
        "# 모든 채널의 평균을 0.5로, 표준편차를 0.5로 설정합니다. 이는 데이터의 범위를 대략적으로 -1.0에서 1.0 사이로 조정\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "# FashionMNIST 데이터셋을 다운로드하고, 지정된 변환을 적용하여 데이터를 준비하는 함수\n",
        "train_val_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# 훈련 및 검증 분할을 위한 데이터 인덱스 생성\n",
        "dataset_size = len(train_val_dataset) # 훈련 및 검증 데이터셋의 전체 크기\n",
        "indices = list(range(dataset_size)) # 데이터셋 내의 모든 샘플에 대한 인덱스를 포함\n",
        "validation_split = 0.1 # 검증 세트로 사용될 데이터의 비율\n",
        "split = int(np.floor(validation_split * dataset_size)) # 검증 세트의 크기를 계산\n",
        "np.random.shuffle(indices) # 훈련 및 검증 세트가 데이터셋의 특정 부분에 치우치지 않도록 하기 위함\n",
        "train_indices, val_indices = indices[split:], indices[:split] # 섞인 인덱스를 사용하여 훈련 세트와 검증 세트의 인덱스를 분할\n",
        "\n",
        "# PT 데이터 샘플러 및 로더 생성\n",
        "# 훈련 세트와 검증 세트에 대한 데이터 로더를 설정하고, 테스트 세트에 대한 데이터 로더를 별도로 설정하는 과정입니다.\n",
        "# 이러한 데이터 로더들은 모델 학습, 검증, 테스트 과정에서 배치 단위로 데이터를 로드하는 데 사용\n",
        "train_sampler = SubsetRandomSampler(train_indices) # train_indices에 해당하는 훈련 데이터의 인덱스를 무작위로 샘플링하는 샘플러를 생성\n",
        "val_sampler = SubsetRandomSampler(val_indices) # val_indices에 해당하는 검증 데이터의 인덱스로부터 데이터를 무작위로 샘플링하는 샘플러를 생성\n",
        "# 배치 크기 4로 로드하는 훈련 데이터 로더를 생성(# 데이터의 무작위 샘플링을 수행)\n",
        "trainloader = torch.utils.data.DataLoader(train_val_dataset, batch_size=4, sampler=train_sampler)\n",
        "valloader = torch.utils.data.DataLoader(train_val_dataset, batch_size=4, sampler=val_sampler)\n",
        "# shuffle=False 인자를 통해 셔플링 없이 순서대로 데이터를 로드. 테스트 과정에서는 데이터의 순서가 결과에 영향을 미치지 않으므로 셔플링을 수행하지 않습니다.\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# 3단계: 네트워크, 손실 함수, 최적화 알고리즘 초기화\n",
        "net = Net() #  클래스의 인스턴스를 생성하여 net 변수에 할당\n",
        "criterion = nn.CrossEntropyLoss() # 멀티클래스 분류 문제에서 널리 사용되는 손실 함수로, 모델의 예측값과 실제 타겟값 사이의 차이를 측정\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # 모멘텀은 최적화 과정에서 이전 그래디언트의 방향을 고려, 파라미터 업데이트 시 관성을 부여\n",
        "\n",
        "# 4단계: 조기 종료를 포함한 모델 학습\n",
        "patience = 5 # 검증 손실이 개선되지 않을 때, 훈련을 계속 진행하기 전에 기다릴 에폭 수를 의미\n",
        "patience_counter = 0\n",
        "best_val_loss = np.Inf # 최고의 검증 손실 값을 무한대로 초기화. 훈련 과정에서 검증 손실이 이전에 기록된 최소 손실보다 낮아지면 업데이트되는 값\n",
        "\n",
        "# 모델을 에폭 단위로 반복 훈련시키면서, 각 배치의 손실을 계산하고 모델 파라미터를 업데이트하는 기본적인 훈련 과정을 구현\n",
        "for epoch in range(20):  # 데이터셋을 여러 번 반복\n",
        "    net.train()  # 모델을 학습 모드로 설정. 이는 모델 내의 특정 레이어(예: 드롭아웃, 배치 정규화 등)가 훈련 시와 평가 시 다르게 동작해야 할 때 필요\n",
        "    running_loss = 0.0 # 현재 에폭의 총 손실을 계산하기 위해 실행 손실을 0으로 초기화\n",
        "    for i, data in enumerate(trainloader, 0): # '0'은 열거의 시작 인덱스를 지정\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad() # 최적화를 수행하기 전에 모델의 그래디언트를 0으로 초기화\n",
        "        outputs = net(inputs) # 현재 배치의 입력 데이터를 모델에 전달하여 예측값을 계산\n",
        "        loss = criterion(outputs, labels) # 모델의 예측값과 실제 레이블 간의 손실을 계산\n",
        "        loss.backward() # 손실 함수의 그래디언트를 역전파합니다. 이 과정에서 모델 파라미터에 대한 손실의 미분값이 계산\n",
        "        optimizer.step() # 계산된 그래디언트를 사용하여 모델의 파라미터를 업데이트\n",
        "        running_loss += loss.item() # 현재 배치의 손실을 실행 손실에 누적. 이를 통해 전체 에폭의 평균 손실을 계산\n",
        "\n",
        "    # 검증 단계\n",
        "    net.eval()  # 모델을 평가 모드로 설정\n",
        "    val_loss = 0.0 # 검증 손실을 계산하기 위한 변수를 0으로 초기화\n",
        "    with torch.no_grad(): # 이 블록 내에서는 그래디언트 계산을 비활성화. 평가 모드에서는 모델을 업데이트하지 않으므로, 그래디언트를 계산할 필요가 없습니다\n",
        "        for inputs, labels in valloader: # 검증 데이터 로더(valloader)에서 배치 단위로 데이터를 로드하여 반복\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels) # criterion은 손실 함수로, 모델의 성능을 측정하는 기준\n",
        "            val_loss += loss.item() # 누적된 검증 손실을 검증 데이터 배치의 총 수로 나누어 평균 검증 손실을 계산\n",
        "    running_loss /= len(trainloader)\n",
        "    val_loss /= len(valloader)\n",
        "    print(f'에폭 {epoch + 1}, 훈련 손실: {running_loss}, 검증 손실: {val_loss}')\n",
        "\n",
        "    # 조기 종료 체크\n",
        "    if val_loss < best_val_loss: # 현재 에폭에서 계산된 검증 손실(val_loss)이 이전에 기록된 최소 검증 손실(best_val_loss)보다 낮은지 확인\n",
        "        print(f'검증 손실이 감소하였습니다. ({best_val_loss:.6f} 에서 {val_loss:.6f}로). 모델 저장 중...')\n",
        "        torch.save(net.state_dict(), '/content/drive/MyDrive/kdt_240424/m6_dl/data/model/best_model.pth')\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print('조기 종료 발동.')\n",
        "            break\n",
        "\n",
        "# 5단계: 최고의 모델을 로드하고 테스트 데이터셋으로 평가\n",
        "# torch.load 함수는 지정된 경로에서 모델 파라미터를 불러오며, load_state_dict 메서드를 사용하여 이 파라미터를 현재 net 모델에 로드\n",
        "net.load_state_dict(torch.load('/content/drive/MyDrive/kdt_240424/m6_dl/data/model/best_model.pth'))\n",
        "correct = 0 # 정확히 분류된 샘플의 수를 세기 위한 변수\n",
        "total = 0 # 테스트셋의 전체 샘플 수를 세기 위한 변수\n",
        "with torch.no_grad():\n",
        "    for data in testloader: # 테스트 데이터셋을 배치 단위로 순회\n",
        "        images, labels = data\n",
        "        outputs = net(images) # 현재 배치의 이미지를 모델에 전달하여 예측값을 계산\n",
        "        _, predicted = torch.max(outputs.data, 1) # torch.max는 각 예측에 대한 최대값과 그 위치(인덱스)를 반환. 위치만 필요하므로 _를 사용하여 최대값은 무시\n",
        "        total += labels.size(0) # labels.size(0)는 현재 배치의 크기(샘플 수)\n",
        "        correct += (predicted == labels).sum().item() # 일치하는 경우의 수를 텐서 형태로 반환하며, .item()으로 이를 파이썬의 스칼라 값으로 변환\n",
        "\n",
        "print(f'10000개의 테스트 이미지에 대한 네트워크의 정확도: {100 * correct / total} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ7zmC52J6vT",
        "outputId": "5d32c94f-25ed-4195-dcde-59ff91fe9b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에폭 1, 훈련 손실: 0.653004357415329, 검증 손실: 0.497642044117713\n",
            "검증 손실이 감소하였습니다. (inf 에서 0.497642로). 모델 저장 중...\n",
            "에폭 2, 훈련 손실: 0.3832080303337781, 검증 손실: 0.3661725013044973\n",
            "검증 손실이 감소하였습니다. (0.497642 에서 0.366173로). 모델 저장 중...\n",
            "에폭 3, 훈련 손실: 0.3320615399670671, 검증 손실: 0.32115839002634555\n",
            "검증 손실이 감소하였습니다. (0.366173 에서 0.321158로). 모델 저장 중...\n",
            "에폭 4, 훈련 손실: 0.30448877325246115, 검증 손실: 0.3230597239609827\n",
            "에폭 5, 훈련 손실: 0.2839316014807367, 검증 손실: 0.3175863423420985\n",
            "검증 손실이 감소하였습니다. (0.321158 에서 0.317586로). 모델 저장 중...\n",
            "에폭 6, 훈련 손실: 0.2691086102266032, 검증 손실: 0.3012078336844649\n",
            "검증 손실이 감소하였습니다. (0.317586 에서 0.301208로). 모델 저장 중...\n",
            "에폭 7, 훈련 손실: 0.25550138932721994, 검증 손실: 0.30303460587284403\n",
            "에폭 8, 훈련 손실: 0.24571981320756842, 검증 손실: 0.3064329963752328\n",
            "에폭 9, 훈련 손실: 0.23613766410586817, 검증 손실: 0.29779855652345333\n",
            "검증 손실이 감소하였습니다. (0.301208 에서 0.297799로). 모델 저장 중...\n",
            "에폭 10, 훈련 손실: 0.22580871239718425, 검증 손실: 0.28825473328034196\n",
            "검증 손실이 감소하였습니다. (0.297799 에서 0.288255로). 모델 저장 중...\n",
            "에폭 11, 훈련 손실: 0.21562310846693059, 검증 손실: 0.3079234967085094\n",
            "에폭 12, 훈련 손실: 0.2106099839552173, 검증 손실: 0.31751980458197593\n",
            "에폭 13, 훈련 손실: 0.20079975541356992, 검증 손실: 0.31585172567316266\n",
            "에폭 14, 훈련 손실: 0.1961117251138252, 검증 손실: 0.3021039334605581\n",
            "에폭 15, 훈련 손실: 0.1891220833783317, 검증 손실: 0.28724593405685156\n",
            "검증 손실이 감소하였습니다. (0.288255 에서 0.287246로). 모델 저장 중...\n",
            "에폭 16, 훈련 손실: 0.18250965658183685, 검증 손실: 0.3093660297465778\n",
            "에폭 17, 훈련 손실: 0.17855608641153614, 검증 손실: 0.3189498388425291\n",
            "에폭 18, 훈련 손실: 0.17164557006760567, 검증 손실: 0.3338968195804638\n",
            "에폭 19, 훈련 손실: 0.16935147384971114, 검증 손실: 0.3497172356734751\n",
            "에폭 20, 훈련 손실: 0.16162400440109007, 검증 손실: 0.34764103081829445\n",
            "조기 종료 발동.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-a3c9f82679c0>:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load('/content/drive/MyDrive/kdt_240424/m6_dl/data/model/best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000개의 테스트 이미지에 대한 네트워크의 정확도: 90.07 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1단계: 신경망 모델 정의\n",
        "class FashionMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 2단계: FashionMNIST 데이터셋 로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# 3단계: 네트워크, 손실 함수, 최적화 알고리즘 초기화\n",
        "model = FashionMNISTModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 4단계: 조기 종료를 포함한 모델 학습 및 best model 저장\n",
        "best_val_loss = float('inf')\n",
        "patience, trials = 5, 0\n",
        "num_epochs = 20\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(trainloader)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(valloader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        trials = 0\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(\"Saved best model\")\n",
        "    else:\n",
        "        trials += 1\n",
        "        if trials >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# 5단계: best model을 로드하고 테스트 데이터셋으로 평가\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "testloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy on the test dataset: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNvtdT9biNuF",
        "outputId": "6eba79b7-4a05-46e9-e41c-008e525693a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:06<00:00, 4330859.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 215619.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3914357.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 4951221.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Epoch 1, Train Loss: 0.453330, Val Loss: 0.323051\n",
            "Saved best model\n",
            "Epoch 2, Train Loss: 0.283177, Val Loss: 0.281939\n",
            "Saved best model\n",
            "Epoch 3, Train Loss: 0.237202, Val Loss: 0.248629\n",
            "Saved best model\n",
            "Epoch 4, Train Loss: 0.203577, Val Loss: 0.246991\n",
            "Saved best model\n",
            "Epoch 5, Train Loss: 0.179454, Val Loss: 0.248894\n",
            "Epoch 6, Train Loss: 0.153368, Val Loss: 0.228347\n",
            "Saved best model\n",
            "Epoch 7, Train Loss: 0.132176, Val Loss: 0.243469\n",
            "Epoch 8, Train Loss: 0.111976, Val Loss: 0.236311\n",
            "Epoch 9, Train Loss: 0.093725, Val Loss: 0.264679\n",
            "Epoch 10, Train Loss: 0.076867, Val Loss: 0.286979\n",
            "Epoch 11, Train Loss: 0.062992, Val Loss: 0.300919\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-84fd25f88ca8>:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test dataset: 91.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task1_0829. 가상 데이터 생성 (generate_data 함수) 후 모델링 및 평가하세요\n",
        "- generate_data 함수는 1000개의 랜덤 시퀀스 데이터를 생성합니다.\n",
        "  - vocab_size: 시퀀스에 사용할 어휘의 크기를 설정합니다. 여기서는 100개의 단어를 사용합니다.\n",
        "  - data: 각 시퀀스는 seq_length=10으로 설정된 10개의 정수(단어 인덱스)로 구성됩니다.\n",
        "  - labels: 각 시퀀스에 대해 0 또는 1의 이진 레이블을 무작위로 할당합니다.\n",
        "\n",
        "- LSTM 기반 분류 모델을 정의하고, 가상 데이터로 학습 및 검증을 수행합니다.\n",
        "- 조기 종료를 통해 학습 중 성능이 더 이상 개선되지 않을 때 학습을 중단합니다.\n",
        "최종적으로 테스트 데이터로 최고 성능의 모델을 평가합니다.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 가상 데이터 생성 함수\n",
        "def generate_data(num_samples=1000, seq_length=10):\n",
        "    vocab_size = 100  # 어휘집 크기\n",
        "    data = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
        "    labels = torch.randint(0, 2, (num_samples,))  # 0 또는 1의 레이블\n",
        "    return data, labels\n",
        "\n",
        "data, labels = generate_data()"
      ],
      "metadata": {
        "id": "yGTpx8-DPmKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### nn.Embedding(vocag_size,embed_dim)\n",
        "- 텍스트 데이터(LSTM 모델): 텍스트 데이터는 이산적인 정수로 표현되므로, 이 정수들을 고차원 벡터로 매핑하는 nn.Embedding 계층이 필요합니다. 이 임베딩 계층은 단어 간의 의미적 유사성을 학습하는 데 유용합니다.\n",
        "\n",
        "- 이미지 데이터(CNN 모델): 이미지 데이터는 이미 공간적 구조를 가진 연속적인 값(픽셀)으로 표현되므로, 임베딩 계층이 필요하지 않습니다. 대신, 합성곱 계층이 이미지의 패턴을 학습하는 데 사용됩니다.\n",
        "\n",
        "LSTM 모델의 경우 텍스트 데이터의 정수 인덱스를 벡터로 변환하기 위해 nn.Embedding이 필요하지만, CNN 모델에서는 이미지 데이터를 처리하는 데 이미 직접적인 합성곱 연산이 사용되므로 임베딩 계층이 필요하지 않습니다."
      ],
      "metadata": {
        "id": "H-e-k7-_nY-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 가상 데이터 생성 함수\n",
        "def generate_data(num_samples=1000, seq_length=10):\n",
        "    vocab_size = 100  # 어휘집 크기\n",
        "    data = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
        "    labels = torch.randint(0, 2, (num_samples,))  # 0 또는 1의 레이블\n",
        "    return data, labels\n",
        "\n",
        "data, labels = generate_data()"
      ],
      "metadata": {
        "id": "IFGAsszeiPen"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 가상 데이터 생성 함수\n",
        "def generate_data(num_samples=1000, seq_length=10):\n",
        "    vocab_size = 100  # 어휘집 크기\n",
        "    data = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
        "    labels = torch.randint(0, 2, (num_samples,))\n",
        "    return data, labels\n",
        "\n",
        "data, labels = generate_data()\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성\n",
        "dataset = TensorDataset(data, labels)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# LSTM 기반 분류 모델 정의\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = output[:, -1, :]\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "# 모델 초기화\n",
        "vocab_size = 100\n",
        "embedding_dim = 32\n",
        "hidden_dim = 64\n",
        "output_dim = 2\n",
        "\n",
        "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "\n",
        "# 손실 함수 및 최적화 알고리즘 설정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 및 검증 함수 정의\n",
        "def train(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return running_loss / len(dataloader), correct / total\n",
        "\n",
        "# 학습 및 검증 수행\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "trials = 0\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_dataloader, criterion, optimizer)\n",
        "    val_loss, val_acc = evaluate(model, val_dataloader, criterion)\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, Val Acc: {val_acc:.6f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        trials = 0\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(\"Saved best model\")\n",
        "    else:\n",
        "        trials += 1\n",
        "        if trials >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# 테스트 데이터로 최고 성능의 모델 평가\n",
        "test_data, test_labels = generate_data(num_samples=200)\n",
        "test_dataset = TensorDataset(test_data, test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
        "print(f\"Test Loss: {test_loss:.6f}, Test Acc: {test_acc:.6f}\")"
      ],
      "metadata": {
        "id": "t9Ayux0YiPcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9330624-c29c-4a99-fa1e-e41c3f76ed9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.693183, Val Loss: 0.698095, Val Acc: 0.470000\n",
            "Saved best model\n",
            "Epoch 2, Train Loss: 0.684583, Val Loss: 0.696612, Val Acc: 0.480000\n",
            "Saved best model\n",
            "Epoch 3, Train Loss: 0.679973, Val Loss: 0.695801, Val Acc: 0.495000\n",
            "Saved best model\n",
            "Epoch 4, Train Loss: 0.671451, Val Loss: 0.698579, Val Acc: 0.470000\n",
            "Epoch 5, Train Loss: 0.663889, Val Loss: 0.708790, Val Acc: 0.465000\n",
            "Epoch 6, Train Loss: 0.654097, Val Loss: 0.708813, Val Acc: 0.490000\n",
            "Epoch 7, Train Loss: 0.640969, Val Loss: 0.727367, Val Acc: 0.460000\n",
            "Epoch 8, Train Loss: 0.629210, Val Loss: 0.737537, Val Acc: 0.470000\n",
            "Early stopping triggered\n",
            "Test Loss: 0.700666, Test Acc: 0.555000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-dd6ac8b42094>:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# 가상 데이터 생성 함수\n",
        "def generate_data(num_samples=1000, seq_length=10):\n",
        "    vocab_size = 100  # 어휘집 크기\n",
        "    data = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
        "    labels = torch.randint(0, 2, (num_samples,))  # 0 또는 1의 레이블\n",
        "    return data, labels\n",
        "\n",
        "data, labels = generate_data()\n",
        "\n",
        "# 텐서 데이터셋 및 데이터 로더 생성\n",
        "dataset = TensorDataset(data, labels)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - (train_size + val_size)\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# LSTM 모델 클래스 정의\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "        hidden = hidden[-1,:,:]\n",
        "        return self.fc(hidden)\n",
        "\n",
        "model = LSTMModel(vocab_size=100, embed_dim=50, hidden_dim=100, output_dim=1)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# 훈련 함수\n",
        "def train(model, train_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data).squeeze(1)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# 평가 함수\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in data_loader:\n",
        "            outputs = model(data).squeeze(1)\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.round(torch.sigmoid(outputs))\n",
        "            correct_predictions = (predictions == labels.unsqueeze(1)).float()\n",
        "            total_accuracy += correct_predictions.sum().item()\n",
        "    return total_loss / len(data_loader), total_accuracy / len(data_loader.dataset)\n",
        "\n",
        "# 조기 종료 로직을 포함한 훈련 및 검증 과정\n",
        "best_val_loss = float('inf') # float('inf')는 파이썬에서 양의 무한대를 나타내는 방식\n",
        "patience = 3\n",
        "trials = 0\n",
        "\n",
        "for epoch in range(20):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        trials = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        trials += 1\n",
        "        if trials >= patience:\n",
        "            print(\"조기 종료 발생\")\n",
        "            break\n",
        "\n",
        "# 테스트 데이터로 최고 모델 평가\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP4OD1L3qff5",
        "outputId": "31a8575c-2523-4a8d-a599-56e2869cef72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.6952, Val Loss: 0.6940, Val Accuracy: 15.1200\n",
            "Epoch 2, Train Loss: 0.6809, Val Loss: 0.6927, Val Accuracy: 15.4267\n",
            "Epoch 3, Train Loss: 0.6671, Val Loss: 0.6936, Val Accuracy: 15.3467\n",
            "Epoch 4, Train Loss: 0.6478, Val Loss: 0.7040, Val Accuracy: 15.3867\n",
            "Epoch 5, Train Loss: 0.6206, Val Loss: 0.7172, Val Accuracy: 15.3333\n",
            "조기 종료 발생\n",
            "Test Loss: 0.6872, Test Accuracy: 15.0800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c109df005c34>:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ]
    }
  ]
}