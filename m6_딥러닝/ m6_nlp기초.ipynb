{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTxK2H7hzI8tyt1l1fTXrH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/windyday0622/windyday/blob/main/m6_%EB%94%A5%EB%9F%AC%EB%8B%9D/%20m6_nlp%EA%B8%B0%EC%B4%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트의 토큰화\n",
        "**자연어 처리(NLP)에서 토큰화(Tokenization)**\n",
        "- 텍스트 분석, 언어 번역, 감정 분석 등 다양한 NLP 작업을 위해 텍스트를 준비하는 기본 단계입니다.\n",
        "- 토큰화는 텍스트를 단어, 문구, 기호 또는 기타 의미 있는 요소(토큰)로 분해하는 작업을 포함합니다. 이 과정에서 생성된 토큰은 추가 처리 및 분석을 위한 기본 구성 요소가 됩니다.\n",
        "\n",
        "토큰화의 목적\n",
        "- 토큰화의 주요 목적은 텍스트 데이터를 단순화하여 알고리즘이 이해하고 처리할 수 있도록 관리하기 쉽게 만드는 것입니다. 이를 통해 텍스트의 복잡성을 줄이고 일관성을 유지함으로써, 다양한 NLP 작업에서 효율적인 분석과 처리가 가능해집니다.\n",
        "\n",
        "토큰화의 유형\n",
        "- 토큰화는 다양한 수준에서 수행될 수 있으며, 각 유형은 특정 NLP 작업의 요구 사항에 따라 선택됩니다:\n",
        "  - 단어 토큰화 (Word Tokenization):\n",
        "    텍스트를 개별 단어로 분해합니다.\n",
        "    예: \"ChatGPT is amazing!\" → [\"ChatGPT\", \"is\", \"amazing\", \"!\"]\n",
        "  - 문장 토큰화 (Sentence Tokenization):\n",
        "    텍스트를 개별 문장으로 분해합니다.\n",
        "    예: \"Hello world. How are you?\" → [\"Hello world.\", \"How are you?\"]\n",
        "  - 하위 단어 토큰화 (Subword Tokenization):\n",
        "    단어를 더 작은 의미 단위로 분해합니다. 주로 BPE(Byte Pair Encoding)나 WordPiece 알고리즘을 사용합니다.\n",
        "    예: \"unhappiness\" → [\"un\", \"hap\", \"pi\", \"ness\"]\n",
        "  - 문자 토큰화 (Character Tokenization):\n",
        "    텍스트를 개별 문자로 분해합니다.\n",
        "    예: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
        "\n",
        "토큰화의 과정\n",
        "- 토큰화 과정은 일반적으로 다음 단계로 구성됩니다:\n",
        "  - 텍스트 정규화 (Text Normalization):\n",
        "    모든 텍스트를 소문자로 변환하여 일관성을 유지합니다.\n",
        "    불필요한 구두점과 공백을 제거합니다.\n",
        "    예: \"Hello, World!\" → \"hello world\"\n",
        "  - 구분자 사용 (Delimiter-based Tokenization):\n",
        "    공백이나 구두점을 기준으로 텍스트를 분해합니다.\n",
        "    예: \"hello world\" → [\"hello\", \"world\"]\n",
        "  - 고급 토큰화 기법 (Advanced Tokenization Techniques):\n",
        "    언어의 문법적, 의미적 구조를 고려하여 토큰을 생성합니다.\n",
        "    BPE, WordPiece, SentencePiece 등의 알고리즘을 사용합니다.\n",
        "\n",
        "토큰화의 중요성\n",
        "- 토큰화는 NLP 작업에서 매우 중요한 역할을 합니다. 잘못된 토큰화는 후속 처리와 분석의 정확도에 큰 영향을 미칠 수 있습니다.\n",
        "- 반면, 올바른 토큰화는 텍스트 데이터를 효과적으로 전처리하고 분석할 수 있게 합니다."
      ],
      "metadata": {
        "id": "CCI3uKp18g5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 고급 토큰화 기법\n",
        "언어의 문법적, 의미적 구조를 고려하여 토큰을 생성하는 것은 텍스트의 의미를 더 잘 보존하고, 더 정확한 분석과 처리를 가능하게 하기 위한 고급 토큰화 기법입니다. 이러한 기법들은 단순히 공백이나 구두점을 기준으로 텍스트를 분해하는 것을 넘어, 단어의 의미와 형태, 문장의 구조 등을 이해하여 더 정교한 토큰을 생성합니다.\n",
        "\n",
        "\n",
        "형태소 분석 (Morphological Analysis):\n",
        "- 단어를 구성하는 최소 의미 단위인 형태소를 분석합니다.\n",
        "- 예를 들어, \"cats\"는 \"cat\"과 복수형 접미사 \"s\"로 분해됩니다.\n",
        "- 형태소 분석기는 단어의 어간과 접사(접두사, 접미사)를 인식하고 분리합니다.\n",
        "\n",
        "어간 추출 (Stemming):\n",
        "- 단어의 어간을 추출하여 형태를 단순화합니다.\n",
        "- 예: \"running\", \"runs\", \"ran\" → \"run\"\n",
        "- 포터 스테머(Porter Stemmer)와 같은 알고리즘이 사용됩니다.\n",
        "\n",
        "어근 추출 (Lemmatization):\n",
        "- 단어의 어근을 추출하여 형태를 표준화합니다. 어간 추출보다 더 정교합니다.\n",
        "- 예: \"running\", \"ran\" → \"run\"\n",
        "- 품사 정보를 사용하여 정확한 어근을 찾아냅니다. 예를 들어, \"better\"는 어근 \"good\"으로 변환됩니다.\n",
        "- WordNetLemmatizer와 같은 도구가 사용됩니다.\n",
        "\n",
        "BPE (Byte Pair Encoding):\n",
        "- 자주 등장하는 바이트 쌍을 병합하여 점진적으로 단어를 분해합니다.\n",
        "- 예: \"lowest\"가 \"l\", \"o\", \"w\", \"e\", \"s\", \"t\"로 분해되고, 자주 등장하는 \"lo\", \"we\"가 결합되어 \"low\", \"est\"로 변환됩니다.\n",
        "- BPE는 신경망 번역 모델과 같은 대규모 언어 모델에서 널리 사용됩니다.\n",
        "\n",
        "WordPiece:\n",
        "- BPE와 유사하지만, 서브워드(subword) 단위로 토큰을 생성합니다.\n",
        "- 예: \"unhappiness\" → [\"un\", \"##happiness\"]\n",
        "- 트랜스포머 모델(BERT 등)에서 사용됩니다.\n",
        "\n",
        "SentencePiece:\n",
        "- 언어에 중립적인 방식으로 텍스트를 서브워드 단위로 분해합니다.\n",
        "- BPE와 유사하지만, 문장을 토큰화하는 과정에서 공백을 고려하지 않음.\n",
        "- 예: \"unhappiness\" → [\"un\", \"ha\", \"ppiness\"]\n",
        "- Google의 T5, ALBERT 모델에서 사용됩니다."
      ],
      "metadata": {
        "id": "ZY6UuZXJ93sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array\n",
        "\n",
        "# 케라스의 텍스트 전처리와 관련한 함수 중 text_to_word_sequence() 함수를 불러옵니다.\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# 전처리할 텍스트를 정합니다.\n",
        "text = '해보지 않으면 해낼 수 없다'\n",
        "\n",
        "# 해당 텍스트를 토큰화합니다.\n",
        "result = text_to_word_sequence(text)\n",
        "print(\"\\n원문:\\n\", text)\n",
        "print(\"\\n토큰화:\\n\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j3jPe1A8osr",
        "outputId": "ab2c8e3e-366b-43b6-d3c3-2bd79590cdf2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "원문:\n",
            " 해보지 않으면 해낼 수 없다\n",
            "\n",
            "토큰화:\n",
            " ['해보지', '않으면', '해낼', '수', '없다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenizer' 클래스는 텍스트를 정수 시퀀스로 변환하도록 설계\n",
        "- fit_on_texts(docs): 이 메소드는 문장 목록(docs)을 인수로 사용하여 token 개체에서 호출된다. 텍스트 목록을 기반으로 내부 어휘를 업데이트하여 토크나이저가 이러한 텍스트로 작업할 수 있도록 준비한다. 말뭉치의 각 고유 단어에 색인을 할당하고 단어 빈도와 같은 다양한 측정항목을 계산하는 작업이 포함된다.\n",
        "- token.word_counts: 토크나이저를 텍스트에 맞춘 후 word_counts는 키가 입력 텍스트에서 발견된 단어이고 값은 각 단어의 발생 횟수인 OrderedDict를 제공한다. 'OrderedDict'를 사용하면 단어가 텍스트에서 처음 나타나는 순서대로 정렬.\n",
        "- token.document_count: 이 속성은 처리된 총 문서(또는 문장) 수를 표시\n",
        "- token.word_docs: word_counts와 유사한 OrderedDict이지만 단어의 빈도 대신 각 단어가 나타나는 문서 수를 표시\n",
        "- token.word_index: 이 속성은 단어를 고유하게 할당된 정수에 매핑하는 OrderedDict를 제공. 모델에는 숫자 입력이 필요하므로 이는 기계 학습 모델의 텍스트를 벡터화하는 데 필요"
      ],
      "metadata": {
        "id": "1MKiFmf5APH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  단어 빈도수 세기\n",
        "\n",
        "# 전처리 하려는 세 개의 문장을 정합니다.\n",
        "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n",
        "        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n",
        "        '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.']\n",
        "\n",
        "# 토큰화 함수를 이용해 전처리 하는 과정입니다.\n",
        "token = Tokenizer()             # 토큰화 함수 지정\n",
        "token.fit_on_texts(docs)        # 토큰화 함수에 문장 적용\n",
        "\n",
        "# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력합니다.\n",
        "# Tokenizer()의 word_counts 함수는 순서를 기억하는 OrderedDict 클래스를 사용합니다.\n",
        "print(\"\\n단어 카운트:\\n\", token.word_counts)\n",
        "print(\"\\n문장 카운트: \", token.document_count)\n",
        "print(\"\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n\", token.word_docs)\n",
        "# token.word_index의 출력 순서는 제공된 텍스트 코퍼스의 각 단어의 빈도에 따라 가장 빈번한 단어부터 가장 빈도가 낮은 단어까지 결정(가장 빈도수가 높은 것부터 출력)\n",
        "# 동일한 빈도의 경우는 먼저 등장한 단어가 더 낮은 인덱스를 할당\n",
        "# 코퍼스 = 말뭉치 = 대규모 텍스트\n",
        "print(\"\\n각 단어에 매겨진 인덱스 값:\\n\", token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp9Qv8mO8oqr",
        "outputId": "724df743-02ff-45cd-e84c-69546e2a2e14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "단어 카운트:\n",
            " OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n",
            "\n",
            "문장 카운트:  3\n",
            "\n",
            "각 단어가 몇 개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'토큰화합니다': 1, '단어를': 1, '먼저': 1, '각': 1, '텍스트의': 2, '나누어': 1, '인식됩니다': 1, '단어로': 1, '토큰화해야': 1, '딥러닝에서': 2, '결과는': 1, '토큰화한': 1, '있습니다': 1, '사용할': 1, '수': 1})\n",
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. 주어진 docs를 토큰화해서 아래 사항을 수행하세요.\n",
        "\n",
        "docs = ['검찰이 제시한 혐의 사실 전부를 재판부가 무죄로 판단하면서 이 회장은 검찰 기소 이후 3년 5개월여 만에 시름을 덜게 됐다.',\n",
        "'검찰 항소로 2심 재판이 진행될 것이란 전망이 나오지만,']"
      ],
      "metadata": {
        "id": "oZCJNb-zC5Fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리할 텍스트를 정합니다.\n",
        "docs = ['검찰이 제시한 혐의 사실 전부를 재판부가 무죄로 판단하면서 이 회장은 검찰 기소 이후 3년 5개월여 만에 시름을 덜게 됐다.',\n",
        "    '검찰 항소로 2심 재판이 진행될 것이란 전망이 나오지만,']\n",
        "\n",
        "# 토큰화 함수를 이용해 전처리 하는 과정입니다.\n",
        "token = Tokenizer()             # 토큰화 함수 지정\n",
        "token.fit_on_texts(docs)        # 토큰화 함수에 문장 적용\n",
        "\n",
        "print(\"\\n단어 카운트:\\n\", token.word_counts)\n",
        "print(\"\\n문장 카운트: \", token.document_count)\n",
        "print(\"\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n\", token.word_docs)\n",
        "print(\"\\n각 단어에 매겨진 인덱스 값:\\n\", token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bceon-LT8ooz",
        "outputId": "156b8130-7629-4c46-80ab-635454d6a662"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "단어 카운트:\n",
            " OrderedDict([('검찰이', 1), ('제시한', 1), ('혐의', 1), ('사실', 1), ('전부를', 1), ('재판부가', 1), ('무죄로', 1), ('판단하면서', 1), ('이', 1), ('회장은', 1), ('검찰', 2), ('기소', 1), ('이후', 1), ('3년', 1), ('5개월여', 1), ('만에', 1), ('시름을', 1), ('덜게', 1), ('됐다', 1), ('항소로', 1), ('2심', 1), ('재판이', 1), ('진행될', 1), ('것이란', 1), ('전망이', 1), ('나오지만', 1)])\n",
            "\n",
            "문장 카운트:  2\n",
            "\n",
            "각 단어가 몇 개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'검찰': 2, '혐의': 1, '시름을': 1, '판단하면서': 1, '이': 1, '덜게': 1, '사실': 1, '검찰이': 1, '재판부가': 1, '기소': 1, '3년': 1, '5개월여': 1, '무죄로': 1, '됐다': 1, '이후': 1, '만에': 1, '제시한': 1, '회장은': 1, '전부를': 1, '진행될': 1, '2심': 1, '재판이': 1, '항소로': 1, '것이란': 1, '나오지만': 1, '전망이': 1})\n",
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'검찰': 1, '검찰이': 2, '제시한': 3, '혐의': 4, '사실': 5, '전부를': 6, '재판부가': 7, '무죄로': 8, '판단하면서': 9, '이': 10, '회장은': 11, '기소': 12, '이후': 13, '3년': 14, '5개월여': 15, '만에': 16, '시름을': 17, '덜게': 18, '됐다': 19, '항소로': 20, '2심': 21, '재판이': 22, '진행될': 23, '것이란': 24, '전망이': 25, '나오지만': 26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras의 Tokenizer를 사용하여 텍스트 데이터를 정수 인덱스 시퀀스로 변환한 후, 이를 One-Hot Encoding 형식으로 변환하는 과정은 NLP 모델의 입력 데이터를 준비하는 중요한 단계입니다.\n",
        "\n",
        "Tokenizer를 사용한 텍스트 토큰화\n",
        "\n",
        "word_index:\n",
        "- token.word_index는 각 단어를 고유한 정수 인덱스로 매핑한 딕셔너리입니다. 키는 단어이고, 값은 해당 단어의 인덱스입니다.\n",
        "- 이 딕셔너리의 길이(len(token.word_index))는 말뭉치에 있는 고유 단어의 총 개수를 나타냅니다.\n",
        "\n",
        "word_size:\n",
        "- word_size는 고유 단어의 총 개수에 1을 더한 값입니다. 이는 NLP에서 일반적인 관행으로, \"0\" 인덱스를 포함하기 위해 사용됩니다.\n",
        "- \"0\" 인덱스는 패딩(padding)에 사용되거나, 구현에 따라 알 수 없는 단어를 나타낼 수 있습니다.\n",
        "\n",
        "One-Hot Encoding:\n",
        "- to_categorical 함수는 클래스 벡터(정수 인덱스)를 바이너리 클래스 행렬로 변환합니다.\n",
        "- x는 단어를 나타내는 정수 인덱스의 목록 또는 배열이어야 하며, - to_categorical은 이를 One-Hot Encoding 형식으로 변환합니다.\n",
        "- 각 정수에 대해 해당 인덱스 위치만 1로 설정되고 나머지 위치는 0인 벡터를 생성합니다.\n",
        "\n",
        "num_classes:\n",
        "- num_classes는 총 클래스 수를 지정합니다. 이 경우 어휘 크기(word_size)로 설정되어, One-Hot Encoding에 어휘의 모든 단어에 대한 슬롯과 추가 \"0\" 인덱스가 있는지 확인합니다."
      ],
      "metadata": {
        "id": "XZcUjbMFF5s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras Tokenizer의 fit_on_tesxts 메소드는 일반적으로 문자열의 리스트를 기대하기 때문에, 단일 문자열을 입력하는 것은 적절하지 않다.\n",
        "text = \"오랫동안 꿈꾸는 이는 그 꿈을 닮아간다\"\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts([text])\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiWLT4EA8omz",
        "outputId": "6f5a0581-2763-476a-db8d-8458398946a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = token.texts_to_sequences([text])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFdiTpBD8ok7",
        "outputId": "2225f791-ccf7-411b-f3eb-5590ec661f00"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4, 5, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ctQsy6hh8oi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ja4AmHHy8og7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vn4JTLIl8ofC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gGIag3si8oc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGcjzHlL8obL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9_o7NRbD8oZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XLq4chQK8oXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5YLK1A0W8oVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yW_wWAWV8oTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsgbZRYq8oRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ApezOnLf8oPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgk8oKZd8PPp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BvWr8diZ8n1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMgYty5b8n3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLxF0Xg58n5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iyl9hYDbGPJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_W9WujaOGPHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BeJHJ1jYGPFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2FwRis0jGPBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1JKntf5QGO_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1gHE5rYGO9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sborq037GO7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lugKjMR7GO5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RN-EJsOGO3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OzmPvFSIGO1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cc0-NknCGOx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwLsYS5FGOv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}