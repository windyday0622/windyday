{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRvsPwsh18R3GHI/7lDxiG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/windyday0622/windyday/blob/main/m6_%EB%94%A5%EB%9F%AC%EB%8B%9D/%20m6_pytorch_%EA%B8%B0%EC%B4%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch vs. Tensorflow/Keras\n",
        "\n",
        "### 1. **PyTorch**\n",
        "\n",
        "#### 1.1. **개요**\n",
        "PyTorch는 Facebook의 AI Research Lab에서 개발한 오픈 소스 딥러닝 프레임워크입니다. Pythonic한 인터페이스와 동적 계산 그래프(dynamic computational graph) 특성 덕분에 학습 곡선이 상대적으로 완만하고, Python 스타일의 코드를 사용할 수 있어서 자연스럽게 익숙해질 수 있습니다. 주로 연구 커뮤니티에서 인기를 얻었고, 최근에는 산업적 사용도 점점 증가하고 있습니다.\n",
        "\n",
        "#### 1.2. **주요 특징**\n",
        "- **동적 계산 그래프 (Dynamic Computation Graph)**\n",
        "  - PyTorch의 가장 큰 특징 중 하나는 동적 계산 그래프입니다. 이를 통해 런타임에 그래프가 생성되므로, 코드 디버깅이나 변경이 훨씬 용이합니다. 이 특성 덕분에 PyTorch는 특히 연구에서 새로운 아이디어를 실험하기에 적합합니다.\n",
        "  \n",
        "- **직관적인 인터페이스**\n",
        "  - Pythonic한 코딩 스타일을 가지고 있으며, 익숙한 Python 문법을 사용합니다. 이로 인해 TensorFlow보다 코드 작성이 상대적으로 간결하고 직관적입니다.\n",
        "  \n",
        "- **TorchScript**\n",
        "  - PyTorch는 TorchScript를 사용하여 Python과 독립적으로 모델을 실행할 수 있게 해줍니다. 이는 모델을 실제 프로덕션 환경에 배포할 때 유용합니다.\n",
        "  \n",
        "- **Autograd**\n",
        "  - PyTorch의 Autograd 기능은 자동 미분을 처리하는데, 이를 통해 백프로퍼게이션 과정에서 파생되는 모든 미분을 자동으로 계산할 수 있습니다.\n",
        "\n",
        "- **커뮤니티**\n",
        "  - 연구 커뮤니티와 긴밀하게 연계되어 있어 많은 논문에서 PyTorch로 구현된 코드가 제공됩니다. 딥러닝 연구에 적합한 프레임워크로 자리 잡고 있습니다.\n",
        "\n",
        "### 2. **TensorFlow / Keras**\n",
        "\n",
        "#### 2.1. **개요**\n",
        "TensorFlow는 Google Brain 팀에서 개발한 오픈 소스 딥러닝 프레임워크로, Keras는 TensorFlow와 통합되어 사용되는 고수준 API입니다. TensorFlow는 생산 환경에서 대규모로 딥러닝 모델을 구축하고 배포하기에 적합하며, Keras는 사용자 친화적인 인터페이스를 제공하여 초보자들도 쉽게 사용할 수 있도록 설계되었습니다.\n",
        "\n",
        "#### 2.2. **주요 특징**\n",
        "- **정적 계산 그래프 (Static Computation Graph)**\n",
        "  - TensorFlow는 초기에는 정적 계산 그래프를 사용했습니다. 즉, 그래프를 먼저 정의하고 나중에 실행하는 방식으로, 한 번 그래프를 정의하면 고정되어 변형이 어렵습니다. 최근에는 TensorFlow 2.0 이후 동적 계산 그래프(Eager Execution)를 도입해 PyTorch처럼 동적 그래프를 지원합니다.\n",
        "  \n",
        "- **확장성과 배포**\n",
        "  - TensorFlow는 대규모 배포 환경에서 매우 강력합니다. TensorFlow Serving, TensorFlow Lite, TensorFlow.js 등 다양한 배포 옵션을 제공하여 모바일, 웹, 임베디드 디바이스까지 모델을 배포할 수 있습니다.\n",
        "  \n",
        "- **Keras (TensorFlow의 고수준 API)**\n",
        "  - Keras는 TensorFlow와 통합된 고수준 API로, 사용자 친화적이고 빠르게 모델을 설계, 구축, 학습할 수 있습니다. 특히 Keras는 초보자들이 쉽게 딥러닝 모델을 시작할 수 있게 해줍니다.\n",
        "  \n",
        "- **TensorBoard**\n",
        "  - TensorFlow는 강력한 시각화 도구인 TensorBoard를 제공합니다. 이를 통해 모델의 학습 과정을 실시간으로 모니터링하고, 그래프 및 파라미터의 변화를 시각적으로 확인할 수 있습니다.\n",
        "\n",
        "- **TensorFlow Hub**\n",
        "  - TensorFlow Hub는 사전 학습된 모델을 재사용할 수 있는 플랫폼으로, 연구자나 개발자들이 이미 학습된 모델을 쉽게 가져와 사용하거나 재학습할 수 있게 도와줍니다.\n",
        "\n",
        "### 3. **PyTorch와 TensorFlow/Keras의 비교**\n",
        "\n",
        "| **비교 항목**           | **PyTorch**                           | **TensorFlow/Keras**                |\n",
        "|------------------------|---------------------------------------|-------------------------------------|\n",
        "| **계산 그래프**         | 동적 계산 그래프 (Dynamic Graph)       | 정적 계산 그래프 (Static Graph) + 동적 지원 (Eager Execution) |\n",
        "| **코딩 스타일**         | Pythonic, 간결하고 직관적             | Keras: 간결, TensorFlow: 다소 복잡함 |\n",
        "| **배포 및 확장성**      | TorchScript로 배포 가능                | TensorFlow Serving, Lite, JS 등 다양한 배포 옵션 |\n",
        "| **자동 미분 (Autograd)**| 매우 직관적                            | 자동 미분 제공하지만, PyTorch만큼 직관적이지 않음 |\n",
        "| **성능**                | 연구 중심, 모델 실험에 적합            | 대규모 배포 환경에서 강력한 성능 제공 |\n",
        "| **커뮤니티**            | 연구 중심, 최신 논문 구현 많이 제공    | 산업계와 연구계 모두 널리 사용됨    |\n",
        "| **시각화 도구**         | TensorBoard 대안은 부족함              | TensorBoard 제공, 강력한 시각화 도구 |\n",
        "| **모바일 및 웹 지원**   | 비교적 미약한 지원                    | TensorFlow Lite, TensorFlow.js 등 폭넓은 지원 |\n",
        "| **프로덕션 환경 지원**  | TorchServe로 배포 가능하지만 한정적     | TensorFlow Extended(TFX)로 대규모 배포 지원 |\n",
        "\n",
        "### 4. **결론**\n",
        "\n",
        "- **PyTorch**는 직관적인 코드 작성과 동적 계산 그래프 덕분에 **연구 및 실험**에 적합합니다. 새로운 아이디어를 빠르게 실험하고 싶거나, 딥러닝 모델을 배우는 데 자연스럽게 접근하고 싶다면 PyTorch가 좋은 선택입니다.\n",
        "- **TensorFlow/Keras**는 **대규모 배포**와 **프로덕션 환경**에서 딥러닝 모델을 사용하는 데 더 적합합니다. 특히 TensorFlow는 다양한 배포 옵션과 확장성을 지원하며, Keras는 간결한 API로 초보자들이 쉽게 접근할 수 있는 장점이 있습니다.\n",
        "\n",
        "둘 다 매우 강력한 딥러닝 프레임워크로, 연구 목적인지, 대규모 배포 목적에 맞는지에 따라 선택할 수 있습니다."
      ],
      "metadata": {
        "id": "m9icPt-wYPtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch의 **동적 계산 그래프 (Dynamic Computation Graph)**는 딥러닝 모델을 구축하고 학습할 때 PyTorch가 제공하는 매우 중요한 기능 중 하나입니다. 이는 \"즉시 실행 (eager execution)\"이라고도 불리며, TensorFlow와 같은 프레임워크의 정적 계산 그래프(static computation graph)와 대비됩니다.\n",
        "\n",
        "### 동적 계산 그래프의 핵심 개념\n",
        "동적 계산 그래프는 학습 중에 그래프를 즉시 정의하고 실행하는 방식입니다. 즉, 계산을 수행할 때마다 새로운 그래프를 만들고, 이를 바로 실행하는 특징을 갖고 있습니다. 이를 통해 계산 과정 중 변수나 모델 구조의 변경이 매우 유연하게 이루어질 수 있습니다.\n",
        "\n",
        "#### 1. **즉시 실행 (Eager Execution)**\n",
        "PyTorch는 코드가 실행될 때마다 계산 그래프를 동적으로 생성하고, 이를 즉시 실행합니다. 즉, 딥러닝 모델에서 데이터를 전방 패스로 전달할 때, 해당 연산이 바로 수행되고 그 결과가 즉시 반환됩니다. 이런 즉시 실행 방식은 디버깅을 매우 용이하게 만들어 줍니다.\n",
        "\n",
        "예를 들어, 다음과 같은 코드를 생각해 봅시다:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "z = x * y + y\n",
        "```\n",
        "\n",
        "여기서 `z`를 계산할 때 `x * y + y`가 바로 실행되고, 이 과정에서 계산 그래프가 동적으로 생성됩니다. 이후 `z.backward()`를 호출하면, PyTorch는 자동으로 그래프를 통해 미분을 수행하여 각 변수의 gradient를 계산합니다.\n",
        "\n",
        "#### 2. **유연성 (Flexibility)**\n",
        "동적 계산 그래프는 모델 구조를 즉석에서 정의하고, 계산 과정 중에도 그래프의 변경이 가능합니다. 이는 복잡한 구조를 가진 모델을 구현할 때 매우 유용합니다. 예를 들어, if문이나 for문을 통해 네트워크의 구조를 유연하게 변경할 수 있습니다.\n",
        "\n",
        "```python\n",
        "for i in range(5):\n",
        "    if i % 2 == 0:\n",
        "        z = z * x\n",
        "    else:\n",
        "        z = z + y\n",
        "```\n",
        "\n",
        "이 코드는 실행할 때마다 그래프의 구조가 바뀔 수 있는데, PyTorch의 동적 계산 그래프는 이러한 경우에도 문제없이 작동합니다. 이는 정적 계산 그래프에서 불가능하거나 복잡한 코드로 구현해야 하는 것을 동적 계산 그래프에서는 자연스럽게 구현할 수 있게 해줍니다.\n",
        "\n",
        "#### 3. **Autograd: 자동 미분 기능**\n",
        "PyTorch에서 동적 계산 그래프는 **Autograd** 기능과 밀접하게 연결되어 있습니다. Autograd는 계산 그래프를 추적하여, 미분이 필요한 연산에 대한 그래프를 자동으로 만들어 줍니다. 연산이 수행되면 PyTorch는 이를 기록하고, `backward()` 함수를 호출할 때 그래프를 따라 역전파를 통해 기울기(gradient)를 계산합니다.\n",
        "\n",
        "```python\n",
        "z.backward()  # z의 연산을 바탕으로 역전파 수행\n",
        "print(x.grad)  # x의 gradient 값 출력\n",
        "```\n",
        "\n",
        "Autograd는 동적으로 그래프를 구성하기 때문에, 계산 그래프는 전방 패스(forward pass) 중에 실시간으로 생성되고, 역전파(backward pass)가 끝나면 필요에 따라 버려집니다. 이는 메모리 효율성을 높이고, 메모리 관리를 쉽게 해줍니다.\n",
        "\n",
        "### 동적 계산 그래프의 장점\n",
        "- **디버깅이 용이함**: 계산이 즉시 실행되므로, 중간 결과를 쉽게 확인할 수 있어 디버깅 과정이 매우 직관적입니다. 오류가 발생하면 즉시 알 수 있습니다.\n",
        "- **유연한 모델 설계**: 반복문, 조건문 등을 사용해 네트워크 구조를 동적으로 변경할 수 있으며, 이는 매우 복잡한 모델을 쉽게 구현하는 데 도움을 줍니다.\n",
        "- **학습 시 효율적**: 필요할 때만 그래프가 생성되고 역전파에 사용되며, 이후 메모리에서 제거되므로 메모리 사용이 효율적입니다.\n",
        "\n",
        "### 정적 계산 그래프와의 차이점\n",
        "- **정적 계산 그래프 (TensorFlow, Theano)**는 모델을 먼저 정의한 후, 그래프 전체를 컴파일하여 실행합니다. 이런 방식은 실행 전에 최적화된 모델을 만들 수 있지만, 그래프를 수정하려면 다시 컴파일해야 하므로 덜 유연합니다.\n",
        "- 반면, **동적 계산 그래프 (PyTorch)**는 실행 시점에서 그래프를 생성하므로 더 유연하지만, 실행 속도는 정적 그래프에 비해 다소 느릴 수 있습니다. 하지만 최근에는 PyTorch에서도 정적 그래프처럼 최적화를 할 수 있는 방법이 연구되고 있습니다(예: TorchScript).\n",
        "\n",
        "### PyTorch의 동적 계산 그래프 작동 방식\n",
        "PyTorch의 동적 계산 그래프는 연산을 할 때마다 다음과 같은 과정을 거칩니다:\n",
        "\n",
        "1. **Tensor와 연산 정의**: 계산 그래프는 Tensor와 연산으로 구성됩니다. Tensor는 `requires_grad=True` 옵션을 통해 그라디언트를 추적할지 여부를 설정할 수 있습니다.\n",
        "2. **전방 패스 실행**: 전방 패스가 실행되면서 연산 결과가 생성되고, 그와 동시에 계산 그래프가 생성됩니다.\n",
        "3. **역전파 실행**: `backward()` 호출 시, 계산 그래프를 타고 내려가면서 그라디언트가 자동으로 계산됩니다.\n",
        "\n",
        "결론적으로, PyTorch의 동적 계산 그래프는 코드가 실행될 때마다 실시간으로 그래프를 구성하여 유연한 모델 설계와 디버깅을 가능하게 합니다. 이 특성 덕분에 PyTorch는 연구나 개발 환경에서 널리 사용되며, 특히 복잡하고 유연한 모델 설계에 적합한 딥러닝 프레임워크로 자리 잡았습니다."
      ],
      "metadata": {
        "id": "R2QU99sVaG2Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K_HK7ucbYLkd"
      },
      "outputs": [],
      "source": [
        "# 여기서 z를 계산할 때 x * y + y가 바로 실행되고, 이 과정에서 계산 그래프가 동적으로 생성.\n",
        "# 이후 z.backward()를 호출하면, PyTorch는 자동으로 그래프를 통해 미분을 수행하여 각 변수의 gradient를 계산합니다.\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "z = x * y + y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 동적 계산 그래프는 모델 구조를 즉석에서 정의하고, 계산 과정 중에도 그래프의 변경이 가능.\n",
        "# if 문이나 for 문을 통해 네트워크의 구조를 유연하게 변경할 수 있습니다.\n",
        "for i in range(5):\n",
        "    if i % 2 == 0:\n",
        "        z = z * x\n",
        "    else:\n",
        "        z = z + y"
      ],
      "metadata": {
        "id": "xHpIW4r8aCdY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# autograd는 계산 그래프를 추적하여, 미분이 필요한 연산에 대해 그래프를 자동으로 만들어줍니다.\n",
        "# 연산이 수행되면 PyTorch는 이를 기록하고, backward() 함수를 호출할 때 그래프를 따라 역전파를 통해 기울기(gradient)를 계산합니다.\n",
        "z.backward() #z의 연산을 바탕으로 역전파 수행\n",
        "print(x.grad)  # x에 대한 gradient 값 출력, z이 x에 대해 어떻게 변환했는지를 나타내는 기울기 값."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvOdJPP6aCaQ",
        "outputId": "2dab8fe9-442b-43fd-e43c-9adea55590b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(147.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "계산 그래프의 사례를 간단한 수식과 함께 살펴볼 수 있습니다. 예를 들어, 다음과 같은 수식을 고려해 보겠습니다:\n",
        "\n",
        "\\[\n",
        "z = (x + y) * w\n",
        "\\]\n",
        "\n",
        "여기서 \\(x\\), \\(y\\), \\(w\\)는 입력 변수이고, \\(z\\)는 결과입니다. 이 수식을 계산 그래프로 나타내면, 각 연산이 그래프의 노드로 표현되고, 변수와 연산 사이의 의존 관계가 엣지로 연결됩니다. 이제 이 수식을 계산 그래프로 표현한 사례를 보여드리겠습니다:\n",
        "\n",
        "### 수식: \\( z = (x + y) * w \\)\n",
        "\n",
        "1. **입력 노드**: \\(x\\), \\(y\\), \\(w\\)는 그래프의 입력 변수입니다.\n",
        "2. **덧셈 연산 노드**: 먼저, \\(x\\)와 \\(y\\)를 더하는 연산이 일어납니다.\n",
        "3. **곱셈 연산 노드**: 그다음, 덧셈 결과와 \\(w\\)를 곱하는 연산이 일어납니다.\n",
        "4. **출력 노드**: 최종 결과인 \\(z\\)가 나옵니다.\n",
        "\n",
        "### 계산 그래프의 시각적 표현\n",
        "다음과 같이 계산 과정을 그래프로 나타낼 수 있습니다:\n",
        "\n",
        "```\n",
        "   x -----\n",
        "          |\n",
        "          + ----> (x + y) -----> * ----> z\n",
        "          |                      |\n",
        "   y -----                       w\n",
        "```\n",
        "\n",
        "여기서 각 단계는 다음과 같습니다:\n",
        "- **덧셈 연산**: \\(x + y\\)는 하나의 노드로 표현됩니다.\n",
        "- **곱셈 연산**: 그다음 덧셈 결과와 \\(w\\)의 곱이 또 다른 노드로 표현됩니다.\n",
        "- **출력**: 최종 결과인 \\(z\\)가 마지막에 나옵니다.\n",
        "\n",
        "### 역전파 과정\n",
        "이 계산 그래프는 **역전파** 과정에서 중요한 역할을 합니다. 예를 들어, 손실 함수에서 \\(z\\)에 대한 기울기를 계산할 때, 연쇄 법칙에 따라 각 연산에 대한 기울기를 차례대로 계산하여 전달합니다.\n",
        "\n",
        "1. **\\(z\\)에 대한 기울기 계산**: 먼저, \\(z\\)에 대한 손실 함수의 기울기를 구합니다.\n",
        "2. **곱셈 연산의 기울기**: \\(z\\)의 기울기를 \\(w\\)와 \\((x + y)\\) 각각에 대해 계산하여 전달합니다.\n",
        "3. **덧셈 연산의 기울기**: \\((x + y)\\)의 기울기를 \\(x\\)와 \\(y\\)에 대해 계산하여 전달합니다.\n",
        "\n",
        "이 과정에서 PyTorch와 같은 프레임워크는 자동으로 역전파를 수행해 각 파라미터에 대한 기울기를 구할 수 있습니다.\n",
        "\n",
        "### 더 복잡한 사례\n",
        "실제 딥러닝 모델에서는 이보다 훨씬 복잡한 계산 그래프가 형성됩니다. 예를 들어, **합성곱 신경망(CNN)**이나 **순환 신경망(RNN)**은 여러 층(layer)과 비선형 활성화 함수, 매트릭스 연산 등이 포함되어 있으며, 이러한 모든 연산이 하나의 거대한 계산 그래프로 연결됩니다. 각 층의 가중치와 입력 간의 연산들이 일련의 계산 그래프를 형성하고, 이 그래프를 통해 역전파가 이루어집니다.\n",
        "\n",
        "---\n",
        "\n",
        "### 요약\n",
        "계산 그래프는 모델의 연산을 시각적으로 표현한 것이며, 이 그래프를 통해 딥러닝 모델의 학습 과정에서 자동 미분과 역전파가 가능합니다."
      ],
      "metadata": {
        "id": "mSbYMAz7dt2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 파이토치의 구성요소"
      ],
      "metadata": {
        "id": "0aOzD1pBgZ01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파이토치의 구성요소\n",
        "\n",
        "- `torch`: 메인 네임스페이스, 텐서 등의 다양한 수학 함수가 포함\n",
        "- `torch.autograd`: 자동 미분 기능을 제공하는 라이브러리\n",
        "- `torch.nn`: 신경망 구축을 위한 데이터 구조나 레이어 등의 라이브러리\n",
        "- `torch.multiprocessing`: 병럴처리 기능을 제공하는 라이브러리\n",
        "- `torch.optim`: SGD(Stochastic Gradient Descent)를 중심으로 한 파라미터 최적화 알고리즘 제공\n",
        "- `torch.utils`: 데이터 조작 등 유틸리티 기능 제공\n",
        "- `torch.onnx`: ONNX(Open Neural Network Exchange), 서로 다른 프레임워크 간의 모델을 공유할 때 사용"
      ],
      "metadata": {
        "id": "cRSlbq6_geNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텐서(Tensors)\n",
        "\n",
        "* 데이터 표현을 위한 기본 구조로 텐서(tensor)를 사용\n",
        "* 텐서는 데이터를 담기위한 컨테이너(container)로서 일반적으로 수치형 데이터를 저장\n",
        "* 넘파이(NumPy)의 ndarray와 유사\n",
        "* GPU를 사용한 연산 가속 가능\n",
        "\n",
        "[ ndarray와 텐서 비교 ]\n",
        "- GPU 지원: PyTorch의 Tensor와 TensorFlow의 텐서는 GPU를 통한 가속을 지원. 이는 대규모 배열 연산에서 상당한 성능 향상을 가져올 수 있는 반면, NumPy ndarray는 기본적으로 CPU에서만 작동.\n",
        "- 자동 미분 지원: PyTorch와 TensorFlow는 딥러닝 모델 학습을 위해 자동 미분을 지원하는 반면, NumPy는 이러한 기능을 내장하고 있지 않다. 이는 신경망의 역전파와 같은 복잡한 연산을 구현할 때 중요한 차이점이다.\n",
        "- API 호환성: PyTorch와 TensorFlow/Keras는 NumPy와의 호환성을 중시하며, 많은 경우에 NumPy의 API를 모방. 이는 NumPy 사용자가 쉽게 이러한 프레임워크로 전환할 수 있도록 돕는다. 그러나 각 프레임워크는 자체적인 최적화와 기능을 제공하기 때문에, 모든 NumPy 연산이 그대로 사용될 수 있는 것은 아니다.\n",
        "\n",
        "[ PyTorch와 TensorFlow/Keras에서의 ndarray 사용 ]\n",
        "- PyTorch에서의 사용: PyTorch는 torch.from_numpy() 함수를 통해 NumPy ndarray를 PyTorch의 Tensor로 변환할 수 있다. 변환된 텐서는 원래 ndarray와 메모리를 공유하기 때문에, 한 객체의 변경이 다른 객체에도 반영된다. 반대로, .numpy() 메소드를 사용하여 PyTorch 텐서를 NumPy 배열로 변환할 수 있다.\n",
        "- TensorFlow/Keras에서의 사용: TensorFlow에서는 tf.convert_to_tensor() 함수를 사용하여 NumPy ndarray를 TensorFlow의 텐서로 변환할 수 있다. 이 변환된 텐서는 TensorFlow 연산에 사용될 수 있다. TensorFlow 텐서를 NumPy 배열로 변환하기 위해서는 .numpy() 메소드를 사용할 수 있다.\n",
        "\n",
        "PyTorch와 TensorFlow/Keras 모두 NumPy ndarray와의 상호 운용성을 지원하며, 이를 통해 각 프레임워크의 텐서로 쉽게 변환할 수 있다. 각 프레임워크의 텐서는 GPU 지원과 자동 미분과 같은 추가적인 기능을 제공하여 딥러닝 모델의 개발과 학습을 효율적으로 할 수 있게 한다."
      ],
      "metadata": {
        "id": "xNKigCkkhS5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3D Tensor\n",
        "\n",
        "* 큐브(cube)와 같은 모양으로 세개의 축이 존재\n",
        "* 데이터가 연속된 시퀀스 데이터나 시간 축이 포함된 시계열 데이터에 해당\n",
        "* 주식 가격 데이터셋, 시간에 따른 질병 발병 데이터 등이 존재\n",
        "* 주로 샘플(samples), 타임스텝(timesteps), 특성(features)을 가진 구조로 사용\n",
        "\n",
        "4D Tensor\n",
        "\n",
        "* 4개의 축\n",
        "* 컬러 이미지 데이터가 대표적인 사례 (흑백 이미지 데이터는 3D Tensor로 가능)\n",
        "* 주로 샘플(samples), 높이(height), 너비(width), 컬러 채널(channel)을 가진 구조로 사용\n",
        "\n",
        "5D Tensor\n",
        "\n",
        "* 5개의 축\n",
        "* 비디오 데이터가 대표적인 사례\n",
        "* 주로 샘플(samples), 프레임(frames), 높이(height), 너비(width), 컬러 채널(channel)을 가진 구조로 사용"
      ],
      "metadata": {
        "id": "lk1i2krMhZuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텐서에 대한 수학 연산, 삼각함수, 비트 연산, 비교 연산, 집계 등 제공\n",
        "import torch\n",
        "\n",
        "a = torch.randn(1, 2) * 2 - 1 # 표준 정규 분포(평균=0, 표준편차=1)에서 가져온 값의 각 요소에 2를 곱한 다음 1을 뺀다.\n",
        "print(a)\n",
        "print(torch.abs(a)) # 절대값\n",
        "print(torch.ceil(a)) # 텐서  a의 각 요소에 천장 함수를 적용\n",
        "print(torch.floor(a)) # 텐서 a의 각 요소에 바닥 함수를 적용\n",
        "print(torch.clamp(a,-0.5,0.5)) # '-0.5' 이하의 값은 '-0.5'로 설정되고, '0.5' 이상의 값은 '0.5'로 설정"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nPToFpSaCXf",
        "outputId": "d4c197cf-7355-4e5f-af55-4ee7d1aa1fa1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-7.4225, -2.3374]])\n",
            "tensor([[7.4225, 2.3374]])\n",
            "tensor([[-7., -2.]])\n",
            "tensor([[-8., -3.]])\n",
            "tensor([[-0.5000, -0.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([1.0, -1.0, 0.5])\n",
        "\n",
        "# 일반 연산(새로운 텐서 생성)\n",
        "y = x.abs()  # 절대값을 구하지만 x는 그대로\n",
        "print(x) # tensor([1.0, -1.0, 0.5])\n",
        "print(y) # tensor([1.0, 1.0, 0.5])\n",
        "\n",
        "# in-place 연산\n",
        "x.abs_()  # 기존 텐서 x의 값을 직접 변경\n",
        "print(x) # tensor([1.0, 1.0, 0.5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbZuGSnzaCUw",
        "outputId": "bed38f80-0e8b-4726-f627-4055c7e299ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.0000, -1.0000,  0.5000])\n",
            "tensor([1.0000, 1.0000, 0.5000])\n",
            "tensor([1.0000, 1.0000, 0.5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2,2) # 0과 1 사이의 값을 가지는 균등분포로부터 무작위로 생성된 행렬을 반환\n",
        "print(x)\n",
        "y = torch.randn(2,2)\n",
        "print(y)\n",
        "print(x)\n",
        "print()\n",
        "y.add_(x) # _가 있는 add_ 매서드는 y 자체를 변경한다는 의미이며 y의 값이 변경\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31_6k3rAaCSv",
        "outputId": "eefef241-40b1-410d-9ca6-f76d3c8f13f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5459, 0.9559],\n",
            "        [0.5011, 0.8086]])\n",
            "tensor([[-0.0559,  0.4168],\n",
            "        [-0.4210, -1.2797]])\n",
            "tensor([[0.5459, 0.9559],\n",
            "        [0.5011, 0.8086]])\n",
            "\n",
            "tensor([[ 0.4900,  1.3727],\n",
            "        [ 0.0800, -0.4710]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.mm: 내적"
      ],
      "metadata": {
        "id": "qrvx8xVAo7LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "print(y,'\\n')\n",
        "print(torch.matmul(x,y)) # 행렬곱\n",
        "z = torch.mm(x,y) # 행렬곱\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Acnt4H5faCRA",
        "outputId": "19b96380-1bae-4ccd-9f0f-9aca2fb564c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5459, 0.9559],\n",
            "        [0.5011, 0.8086]])\n",
            "tensor([[ 0.4900,  1.3727],\n",
            "        [ 0.0800, -0.4710]]) \n",
            "\n",
            "tensor([[0.3440, 0.2991],\n",
            "        [0.3103, 0.3069]])\n",
            "tensor([[0.3440, 0.2991],\n",
            "        [0.3103, 0.3069]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "인덱싱: NumPy 처럼 인덱싱 형태로 사용가능"
      ],
      "metadata": {
        "id": "FAFgvtSuo_1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.Tensor([[1,2],[3,4]])\n",
        "print(x)\n",
        "\n",
        "print(x[0,0])\n",
        "print(x[0,1])\n",
        "print(x[1,0])\n",
        "print(x[1,1])\n",
        "print(x[:,0])\n",
        "print(x[:,1])\n",
        "print(x[0,:])\n",
        "print(x[1,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8iZIurOaCO_",
        "outputId": "250a4a7c-3c17-4fec-fac7-dd32a6ea7dde"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "tensor(1.)\n",
            "tensor(2.)\n",
            "tensor(3.)\n",
            "tensor(4.)\n",
            "tensor([1., 3.])\n",
            "tensor([2., 4.])\n",
            "tensor([1., 2.])\n",
            "tensor([3., 4.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "º 랜덤한 값을 가지는 텐서 생성\n",
        "\n",
        "1. torch.rand() : 0과 1 사이의 숫자를 균등하게 생성\n",
        "\n",
        "2. torch.rand_like() : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
        "\n",
        "3. torch.randn() : 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용해 생성\n",
        "\n",
        "4. torch.randn_like() :  사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
        "\n",
        "5. torch.randint() : 주어진 범위 내의 정수를 균등하게 생성\n",
        "\n",
        "6. torch.randint_like() : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
        "\n",
        "7. torch.randperm() : 주어진 범위 내의 정수를 랜덤하게 생성"
      ],
      "metadata": {
        "id": "oNBysgXerT3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a tensor with random values\n",
        "tensor_rand = torch.rand(3, 3) # 1\n",
        "existing_tensor = torch.tensor([[1,2],[3,4]])\n",
        "tensor_rand_like = torch.randn_like(existing_tensor, dtype=torch.float) # 2\n",
        "tensor_randn = torch.randn(3, 3) # 3\n",
        "tensor_randn_like = torch.randn_like(existing_tensor, dtype=torch.float) # 4\n",
        "tensor_ranint = torch.randint(0, 10, (3, 3)) # 5\n",
        "tensor_ranint_like = torch.randint_like(existing_tensor, low=0, high=10, dtype=torch.int) # 6\n",
        "tensor_randperm = torch.randperm(10) # 0 부터 n-1까지의 정수를 무작위로 섞은 순열을 반환, 중복 없는 무작위 정수 시퀀스.\n",
        "\n",
        "tensor_rand, tensor_rand_like, tensor_randn, tensor_randn_like, tensor_ranint, tensor_ranint_like, tensor_randperm,"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIARcCIyaCMv",
        "outputId": "a3f37aec-a120-46a0-ebb3-f9bd34fc1a2c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0170, 0.1065, 0.0577],\n",
              "         [0.1244, 0.1972, 0.7401],\n",
              "         [0.1818, 0.6895, 0.2429]]),\n",
              " tensor([[-0.1458, -1.9910],\n",
              "         [ 0.5910,  0.7741]]),\n",
              " tensor([[-1.1138, -0.5286, -0.4805],\n",
              "         [-1.2169, -0.7656, -1.4156],\n",
              "         [ 0.0495, -1.1863, -0.4858]]),\n",
              " tensor([[ 0.2468, -0.3030],\n",
              "         [ 0.0847,  0.0586]]),\n",
              " tensor([[5, 5, 6],\n",
              "         [3, 3, 6],\n",
              "         [5, 1, 2]]),\n",
              " tensor([[3, 0],\n",
              "         [0, 7]], dtype=torch.int32),\n",
              " tensor([2, 3, 9, 6, 5, 8, 0, 1, 4, 7]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "º 특정한 값을 가지는 텐서 생성\n",
        "\n",
        "1. torch.arange() : 주어진 범위 내의 정수를 순서대로 생성\n",
        "\n",
        "2. torch.ones() : 주어진 사이즈의 1로 이루어진 텐서 생성\n",
        "\n",
        "3. torch.zeros() : 주어진 사이즈의 0으로 이루어진 텐서 생성\n",
        "\n",
        "4. torch.ones_like() : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
        "\n",
        "5. torch.zeros_like() : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
        "\n",
        "6. torch.linspace() : 시작점과 끝점을 주어진 갯수만큼 균등하게 나눈 간격점을 행벡터로 출력\n",
        "\n",
        "7. torch.logspace() : 시작점과 끝점을 주어진 갯수만큼 로그간격으로 나눈 간격점을 행벡터로 출력"
      ],
      "metadata": {
        "id": "iMFqNMCdsd0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a tensor with a specific value\n",
        "tensor_arange = torch.arange(0, 10)\n",
        "tensor_ones = torch.ones(3, 3)\n",
        "tensor_zeros = torch.zeros(3, 3)\n",
        "tensor_ones_like = torch.ones_like(existing_tensor)\n",
        "tensor_zeros_like = torch.zeros_like(existing_tensor)\n",
        "tensor_linspace = torch.linspace(0, 10, steps=5)\n",
        "tensor_logspace = torch.logspace(start=-1, end=1, steps=5)\n",
        "\n",
        "# Output for demonstration\n",
        "tensor_arange, tensor_ones, tensor_zeros, tensor_ones_like, tensor_zeros_like, tensor_linspace, tensor_logspace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DKH9GZwaCKP",
        "outputId": "6a18e613-d6f9-4a94-f654-5685482f0ef0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
              " tensor([[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]]),\n",
              " tensor([[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]]),\n",
              " tensor([[1, 1],\n",
              "         [1, 1]]),\n",
              " tensor([[0, 0],\n",
              "         [0, 0]]),\n",
              " tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000]),\n",
              " tensor([ 0.1000,  0.3162,  1.0000,  3.1623, 10.0000]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`view`: 텐서의 크기(size)나 모양(shape)을 변경\n",
        "\n",
        "- 기본적으로 변경 전과 후에 텐서 안의 원소 개수가 유지되어야 함\n",
        "- -1로 설정되면 계산을 통해 해당 크기값을 유추"
      ],
      "metadata": {
        "id": "jAqXfVT-uPDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(4,5)\n",
        "print(x, '\\n')\n",
        "\n",
        "y = x.view(20) # view() 함수는 텐서의 크기를 변경\n",
        "print(y, '\\n')\n",
        "z = x.view(5,-1) # -1은 다른 차원에서 계산된 값에 따라 자동으로 결정\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBY7cv88aCHn",
        "outputId": "94acd720-dd2a-44e2-a74d-ef74b9e4ec22"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3070,  1.7022,  0.4849,  0.6610, -0.3172],\n",
            "        [ 1.8159,  0.1946, -0.3486, -0.4460, -0.6909],\n",
            "        [-0.2292,  0.7246, -0.1984,  1.0816, -0.2144],\n",
            "        [-0.3890, -0.1636, -0.2202,  1.7115, -0.7940]]) \n",
            "\n",
            "tensor([-0.3070,  1.7022,  0.4849,  0.6610, -0.3172,  1.8159,  0.1946, -0.3486,\n",
            "        -0.4460, -0.6909, -0.2292,  0.7246, -0.1984,  1.0816, -0.2144, -0.3890,\n",
            "        -0.1636, -0.2202,  1.7115, -0.7940]) \n",
            "\n",
            "tensor([[-0.3070,  1.7022,  0.4849,  0.6610],\n",
            "        [-0.3172,  1.8159,  0.1946, -0.3486],\n",
            "        [-0.4460, -0.6909, -0.2292,  0.7246],\n",
            "        [-0.1984,  1.0816, -0.2144, -0.3890],\n",
            "        [-0.1636, -0.2202,  1.7115, -0.7940]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'item' : 텐서에 값이 단 하나라도 존재하면 숫자값을 얻을 수 있음.\n",
        "x = torch.randn(1)\n",
        "print(x)\n",
        "print(x.item()) # item() 함수는 텐서의 값을 Python 숫자로 반환하며 텐서에서 값만 추출\n",
        "print(x.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI6fcqLEaCFB",
        "outputId": "0b352148-3e9d-4791-890f-b638d7c6e251"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2130])\n",
            "0.2130080759525299\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "squeeze() 함수는 크기가 1인 입력 텐서의 모든 차원을 제거하는 데 사용\n",
        "- dim이 제공되지 않으면 squeeze()는 텐서에서 크기 1의 모든 차원을 제거\n",
        "- dim이 제공되면 크기가 1인 경우에만 지정된 차원만 제거. 즉 (1, 2, 1, 3)인 텐서가 있고 dim이 2이면 결과의 크기는 (1, 2, 3)"
      ],
      "metadata": {
        "id": "_tvFhK7tvDA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'squeeze' : 차원을 축수(제거)\n",
        "tensor = torch.rand(1, 3, 3)\n",
        "print(tensor)\n",
        "print(tensor.shape)\n",
        "t = tensor.squeeze()\n",
        "print(t)\n",
        "print(t.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf_6R9L5aCCX",
        "outputId": "f4cdedbe-8ba4-4f9a-b904-d3f40931b4f0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3570, 0.2035, 0.0731],\n",
            "         [0.7569, 0.4327, 0.8649],\n",
            "         [0.1795, 0.7521, 0.4555]]])\n",
            "torch.Size([1, 3, 3])\n",
            "tensor([[0.3570, 0.2035, 0.0731],\n",
            "        [0.7569, 0.4327, 0.8649],\n",
            "        [0.1795, 0.7521, 0.4555]])\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensor.unsqueeze(dim=2)\n",
        "\n",
        "- unsqueeze 함수는 텐서에 새로운 차원을 추가. 여기서 dim=2는 새로운 차원이 추가될 위치를 나타내며 기존 텐서의 모든 차원의 인덱스를 증가시키고, 새로운 차원은 dim=2에 위치.\n",
        "\n",
        "- 원래 텐서의 모양(shape)이 (3, 4)이었다면, unsqueeze(dim=2)를 적용한 후의 모양은 (3, 4, 1)이 됩니다. 따라서, 이 코드에서는 기존 텐서에 1차원을 추가"
      ],
      "metadata": {
        "id": "WniYD1YBvr3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.rand(3, 3)\n",
        "print(t)\n",
        "print(t.shape)\n",
        "tensor = t.unsqueeze(dim=0)\n",
        "print(tensor)\n",
        "print(tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaYICC7NaB_o",
        "outputId": "c76308a7-5078-4d93-e043-3412f66ce617"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0276, 0.2011, 0.7821],\n",
            "        [0.4610, 0.8337, 0.9031],\n",
            "        [0.9554, 0.7572, 0.4099]])\n",
            "torch.Size([3, 3])\n",
            "tensor([[[0.0276, 0.2011, 0.7821],\n",
            "         [0.4610, 0.8337, 0.9031],\n",
            "         [0.9554, 0.7572, 0.4099]]])\n",
            "torch.Size([1, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY-2_2plaB8g",
        "outputId": "d3f9309b-fb23-4eff-eacb-725e11a36834"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = tensor.unsqueeze(dim=2)\n",
        "print(tensor)\n",
        "print(tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3EeIpTdaB6Q",
        "outputId": "bed40777-df4e-4c93-bdfb-09a787bdcb8a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0276, 0.2011, 0.7821]],\n",
            "\n",
            "         [[0.4610, 0.8337, 0.9031]],\n",
            "\n",
            "         [[0.9554, 0.7572, 0.4099]]]])\n",
            "torch.Size([1, 3, 1, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. 아래 텐서를 torch.Size([3,5])로 변경하세요"
      ],
      "metadata": {
        "id": "Du-J9T6n2kdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 초기 텐서 생성\n",
        "tensor_for_squeeze = torch.randn(1, 3, 1, 5)\n",
        "print(\"차원 축소 전 텐서 (squeeze 전):\", tensor_for_squeeze)\n",
        "print(\"차원 축소 전 형태\", tensor_for_squeeze.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvXeTabsaB3w",
        "outputId": "19e5d5c8-e480-440e-8109-60a350cc8ce0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "차원 축소 전 텐서 (squeeze 전): tensor([[[[-0.0995, -0.2563, -1.6806,  0.3176, -0.2511]],\n",
            "\n",
            "         [[ 1.2418, -0.2236,  2.1539,  0.4396,  1.1434]],\n",
            "\n",
            "         [[-2.1019,  0.2155, -1.8998,  0.7468,  0.2406]]]])\n",
            "차원 축소 전 형태 torch.Size([1, 3, 1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.squeeze()는 모든 크기 1인 차원을 자동으로 제거하지만, 특정 차원만 제거하고 싶다면 차원을 지정할 수도 있습니다.\n",
        "squeeze_tensor = torch.squeeze(tensor_for_squeeze)\n",
        "print(squeeze_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNTJYYTqaBzY",
        "outputId": "b88fb040-e30b-4e29-81e5-e23e641e8a4a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. 아래 텐서를 torch.Size([1,3,5])로 변경하세요."
      ],
      "metadata": {
        "id": "5F4fInUd3guT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 초기 텐서 생성\n",
        "tensor_for_unsqueeze = torch.randn(3,5)\n",
        "print(\"차원 축소 전 텐서 (squeeze 전):\", tensor_for_unsqueeze)\n",
        "print(\"차원 축소 전 형태\", tensor_for_unsqueeze.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dzYYjGfaBxQ",
        "outputId": "914521bc-6c7b-4889-c4e4-f05e9771fe67"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "차원 축소 전 텐서 (squeeze 전): tensor([[-0.2437, -2.0381,  1.3812,  1.0127,  0.4539],\n",
            "        [-0.1267,  2.6175, -0.1964,  1.0544,  1.7656],\n",
            "        [-0.2522, -1.1112, -1.0110,  1.2428,  0.7256]])\n",
            "차원 축소 전 형태 torch.Size([3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unsqueezed_tensor = torch.unsqueeze(tensor_for_unsqueeze, 0)\n",
        "print(unsqueezed_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFs4YMlmaBuo",
        "outputId": "316a795e-8a17-422a-a030-3a466ac81fd2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task1_0827. 2행 3열 텐서의 크기를 3행 2열로 변경하세요.\n",
        "- view 사용\n",
        "- reshape 사용\n",
        "- transpose 사용"
      ],
      "metadata": {
        "id": "E_EA0j-_34zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 크기가 2x3인 텐서를 생성합니다.\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# view를 사용하여 텐서의 크기를 3x2로 변경합니다.\n",
        "reshaped_tensor_view = tensor.view(3, 2)\n",
        "\n",
        "# reshape를 사용하여 텐서의 크기를 3x2로 변경합니다.\n",
        "reshaped_tensor_reshape = tensor.reshape(3, 2)\n",
        "\n",
        "# transpose를 사용하여 텐서의 차원을 교환합니다.\n",
        "transposed_tensor = tensor.transpose(0, 1)\n",
        "\n",
        "reshaped_tensor_view, reshaped_tensor_reshape, transposed_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKG7mAzZaBqg",
        "outputId": "8b43f947-6a87-4180-b4a8-676fe8e6db33"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1, 2],\n",
              "         [3, 4],\n",
              "         [5, 6]]),\n",
              " tensor([[1, 2],\n",
              "         [3, 4],\n",
              "         [5, 6]]),\n",
              " tensor([[1, 4],\n",
              "         [2, 5],\n",
              "         [3, 6]]))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}